{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from helper import lineNotify as LN\n",
    "from helper import writeProgress, newPath, write_json, read_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 2372)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_2372.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre_like.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n",
      "150 82\n",
      "64 2372\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "print(usr_nb, movie_nb)\n",
    "\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 82\n",
    "print(usr_test_amount, movie_test_amount)\n",
    "\n",
    "latent_dim = 64 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "print(latent_dim, ft_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n",
      "Before: [[ 2  2  0 ...  1  0  0]\n",
      " [34 49 14 ...  1  0  0]\n",
      " [10 17 17 ...  4  0  0]\n",
      " ...\n",
      " [79 43  0 ...  8 12  0]\n",
      " [ 6  6  0 ...  0  1  0]\n",
      " [67 53  1 ...  3  5  0]]\n",
      "After: [[0.05263158 0.05263158 0.         ... 0.02631579 0.         0.        ]\n",
      " [0.41463415 0.59756098 0.17073171 ... 0.01219512 0.         0.        ]\n",
      " [0.24390244 0.41463415 0.41463415 ... 0.09756098 0.         0.        ]\n",
      " ...\n",
      " [0.30620155 0.16666667 0.         ... 0.03100775 0.04651163 0.        ]\n",
      " [0.24       0.24       0.         ... 0.         0.04       0.        ]\n",
      " [0.74444444 0.58888889 0.01111111 ... 0.03333333 0.05555556 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)\n",
    "print('Before:', usr_genre)\n",
    "print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "Avg of followers: 142.0969696969697\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "print('Avg of followers:', np.mean(moive_followers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "Avg of followers: 14.820480404551201\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "print('Avg of followers:', np.mean(each_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10])\n",
    "# 150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "train_t = []\n",
    "train_f = []\n",
    "test_t = []\n",
    "test_f = []\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    # init\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    t_for_test = []\n",
    "    f_for_test = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "#         print(len(t_for_train) + len(f_for_train))\n",
    "        \n",
    "    else: #if in test id, choose half of true and other \n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #the others for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    if not (len(t_for_train) + len(f_for_train) + len(t_for_test) + len(f_for_test)) == movie_nb:\n",
    "        print('Error!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train/test split\n",
    "SPLIT_PATH = './split/LHO_82_no20/'\n",
    "train_t = read_json(SPLIT_PATH + 'train_t.json')\n",
    "train_f = read_json(SPLIT_PATH + 'train_f.json')\n",
    "test_t = read_json(SPLIT_PATH + 'test_t.json')\n",
    "test_f = read_json(SPLIT_PATH + 'test_f_82.json')\n",
    "movie_test_amount = 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load train/test split\n",
    "# SPLIT_PATH = './split/original_testing/'\n",
    "# train_t = read_json(SPLIT_PATH + 'train_t.json')\n",
    "# train_f = read_json(SPLIT_PATH + 'train_f.json')\n",
    "# test_t = read_json(SPLIT_PATH + 'test_t.json')\n",
    "# test_f = read_json(SPLIT_PATH + 'test_f.json')\n",
    "# movie_test_amount = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTraining: 14.139064475347661\\nTesting: 7.1866666666666665\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)\n",
    "'''\n",
    "Training: 14.139064475347661\n",
    "Testing: 7.1866666666666665\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top N\n",
    "def F1_score(prec,rec):\n",
    "    f1 = 2*((prec*rec)/(prec+rec))\n",
    "    return f1\n",
    "\n",
    "def topN(RSls, n):\n",
    "    maxn = np.argsort(RSls)[::-1][:n]\n",
    "    return maxn\n",
    "\n",
    "# NDCG\n",
    "def allSortPrepare(testRS):\n",
    "    all_sort = []\n",
    "\n",
    "    for i in range(usr_test_amount):\n",
    "        all_sort.append(topN(list(testRS[i]),len(testRS[i])))\n",
    "\n",
    "    all_sort = np.asarray(all_sort)\n",
    "    print(all_sort.shape)\n",
    "    return all_sort\n",
    "\n",
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    return dcg\n",
    "\n",
    "def NDCG(target, testRS, num_ndcg, all_sort): #target是真正的喜好\n",
    "    total_ndcg = []\n",
    "    \n",
    "    for m in range(usr_test_amount): # the number of testing users\n",
    "        idcg = DCG(target[m][:num_ndcg])\n",
    "#         print('target[m][:num_ndcg]:\\n', target[m][:num_ndcg])\n",
    "        \n",
    "        pre_list = []\n",
    "        for s in all_sort[m][:num_ndcg]:\n",
    "            #print(m,s,target[m][s])\n",
    "#             print('target[m][:num_ndcg]:\\n', target[m][:num_ndcg])\n",
    "            pre_list.append(target[m][s]) #把prec_list 的 score加進去\n",
    "#         print('pre_list:\\n', pre_list)\n",
    "        dcg = DCG(pre_list)\n",
    "#         print('dcg:', dcg)\n",
    "#         print('idcg:', idcg)\n",
    "#         print('=====')\n",
    "        if idcg == 0:\n",
    "#             ndcg = 0\n",
    "#             total_ndcg.append(ndcg)\n",
    "            pass\n",
    "        else:\n",
    "            ndcg = dcg/idcg\n",
    "            total_ndcg.append(ndcg)\n",
    "#         total_ndcg += ndcg\n",
    "    \n",
    "    avg_ndcg = np.mean(total_ndcg)\n",
    "#     print('len(total_ndcg):', len(total_ndcg))\n",
    "#     print('avg(total_ndcg):', avg_ndcg)\n",
    "    return avg_ndcg\n",
    "\n",
    "# MAP\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def MAP(target,testRS):\n",
    "    total_prec = 0\n",
    "    for u in range(usr_test_amount):\n",
    "        y_true = target[u]\n",
    "        y_scores = testRS[u]\n",
    "        total_prec += average_precision_score(y_true, y_scores)\n",
    "        \n",
    "    Map_value = total_prec/usr_test_amount\n",
    "    \n",
    "    return Map_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(testRS, target, sumtarget, all_sort):\n",
    "    msg = ''\n",
    "    print('\\n==============================\\n')\n",
    "    # Top N\n",
    "    N = [1, 3, 5]\n",
    "\n",
    "    for n in N:\n",
    "        print('Top', n)\n",
    "        correct = 0\n",
    "\n",
    "        for i in range(len(testRS)):\n",
    "            topn = topN(testRS[i], n)\n",
    "            sum_target = int(np.sum(target[i]))\n",
    "\n",
    "            TP = 0\n",
    "            for i in topn:\n",
    "                if i < sum_target:\n",
    "                    TP += 1\n",
    "\n",
    "            correct += TP\n",
    "\n",
    "        prec = correct/(len(testRS)*n) #150*n\n",
    "        recall = correct/sumtarget\n",
    "        print('TP:', correct)\n",
    "        print('prec:', prec)\n",
    "        print('recall:', recall)\n",
    "        print('F1_score:', F1_score(prec, recall))\n",
    "        print('*****')\n",
    "        msg += '\\nTop {}\\nTP: {}\\nprec: {}\\nrecall: {}\\nF1_score: {}\\n'.format(n, correct, prec, recall, F1_score(prec, recall))\n",
    "    print('\\n==============================\\n')\n",
    "\n",
    "    # NDCG\n",
    "    num_ndcgs = [10]\n",
    "    for num_ndcg in num_ndcgs:\n",
    "        print('NDCG@', num_ndcg)\n",
    "        print('NDCG score:', NDCG(target, testRS, num_ndcg, all_sort))\n",
    "        print('*****')\n",
    "        msg += '\\nNDCG@{}: {}'.format(num_ndcg, NDCG(target, testRS, num_ndcg, all_sort))\n",
    "    print('\\n==============================\\n')\n",
    "    \n",
    "    LN(msg)\n",
    "#     # MAP\n",
    "#     print('MAP:', MAP(target,testRS))\n",
    "#     print('\\n==============================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def testing(U, Y, A, E, Au, Ay, Aa, Av, B):\n",
    "    #with Embedding\n",
    "    result = np.zeros((usr_test_amount, movie_nb))\n",
    "    RS = np.zeros((usr_test_amount, movie_nb))\n",
    "\n",
    "    #test_idx --> Test 的 index length = 150\n",
    "    sum_alpha = 0\n",
    "    test_yes_id = []\n",
    "\n",
    "    for s in range(usr_test_amount):\n",
    "#         sample = train_t[test_idx[s]]\n",
    "        sample = [i for i in range(movie_nb)]\n",
    "        alpha = np.zeros([len(sample)])\n",
    "\n",
    "        for a in range(len(sample)):\n",
    "            r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "\n",
    "            alpha_a = (np.dot(Au[test_idx[s]][sample[a]],np.expand_dims(U[test_idx[s]],0).T) + \n",
    "                       np.dot(Ay[test_idx[s]][sample[a]],np.expand_dims(Y[sample[a]],0).T) + \n",
    "                       np.dot(Aa[test_idx[s]][sample[a]],np.expand_dims(A[sample[a]],0).T) +\n",
    "                       np.dot(Av[test_idx[s]][sample[a]],np.dot(E,np.expand_dims(all_npy[sample[a]],0).T)))\n",
    "\n",
    "\n",
    "            # relu part\n",
    "            alpha[a]=np.sum((relu(alpha_a)))*r\n",
    "            # tanh part\n",
    "    #         alpha[a]=np.sum((np.tanh(alpha_a)))*r\n",
    "\n",
    "        mul = np.zeros((1,latent_dim))\n",
    "        added_alpha = np.add(alpha,0.0000000001)\n",
    "        norm_alpha = added_alpha/np.sum(added_alpha)\n",
    "        sum_alpha += np.sum(alpha)\n",
    "\n",
    "        for i in range(len(sample)):\n",
    "            mul += norm_alpha[i] * A[sample[i]] # attention alpha*Ai part\n",
    "        new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "\n",
    "        for k in range(movie_nb):\n",
    "            result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "            RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))\n",
    "        \n",
    "    #取出test的資料\n",
    "    print('RS:', RS.shape)\n",
    "\n",
    "    testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "    target = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "\n",
    "    for z in range(usr_test_amount):\n",
    "        user_id = test_idx[z]\n",
    "        # positive target YouTuber list\n",
    "        youtube_t = test_t[z] \n",
    "        # not target YouTuber list\n",
    "        youtube_f = test_f[z]\n",
    "\n",
    "#         print(user_id)\n",
    "#         print(youtube_t)\n",
    "#         print(youtube_f)\n",
    "\n",
    "        #前面放target的RS\n",
    "        for i in range(len(youtube_t)):\n",
    "            testRS[z][i] = RS[z][youtube_t[i]]\n",
    "            target[z][i] = 1\n",
    "\n",
    "        for i in range(len(youtube_f)):\n",
    "            testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "\n",
    "    #     print(testRS[z])\n",
    "    #     print(target[z])\n",
    "    #     print('==============================')\n",
    "\n",
    "    print(target.shape, testRS.shape)\n",
    "    sumtarget = np.sum(target)\n",
    "    print('num of positive data in testing:', sumtarget) # whole matrix: 4800\n",
    "\n",
    "    # for metrics\n",
    "    metrics(testRS, target, sumtarget, allSortPrepare(testRS))\n",
    "    \n",
    "    return RS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def changes(SAVE_DIR, SAVE_NAME):\n",
    "    print(SAVE_NAME)\n",
    "    LN(SAVE_NAME)\n",
    "    params = np.load(SAVE_DIR + SAVE_NAME)\n",
    "#     print(params)\n",
    "    U = params['U']\n",
    "    Y = params['Y']\n",
    "    A = params['A']\n",
    "    E = params['E']\n",
    "    Au = params['Wu']\n",
    "    Ay = params['Wy']\n",
    "    Aa = params['Wa']\n",
    "    Av = params['Wv']\n",
    "    B = params['B']\n",
    "\n",
    "#     print('User latent shape: ',U.shape)\n",
    "#     print('photo latent shape: ', Y.shape)\n",
    "#     print('Auxilary latent shape: ',A.shape)\n",
    "#     print('Embedding shape:', E.shape)\n",
    "#     print('Wu weight shape:', Au.shape)\n",
    "#     print('Wy weight shape:', Ay.shape)\n",
    "#     print('Wa weight shape:', Aa.shape)\n",
    "#     print('Wv weight shape:', Av.shape)\n",
    "#     print('Beta shape:',B.shape)\n",
    "    \n",
    "    return testing(U, Y, A, E, Au, Ay, Aa, Av, B)\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load train/test split\n",
    "# SPLIT_PATH = './split/original_training/'\n",
    "# train_t = read_json(SPLIT_PATH + 'train_t.json')\n",
    "# train_f = read_json(SPLIT_PATH + 'train_f.json')\n",
    "# SPLIT_PATH = './split/original_testing/'\n",
    "# test_t = read_json(SPLIT_PATH + 'test_t.json')\n",
    "# test_f = read_json(SPLIT_PATH + 'test_f.json')\n",
    "# movie_test_amount = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files: ['.ipynb_checkpoints', 'MRM_E240_1e-4_5e-4_no33.npz', 'MRM_E240_1e-5_no20.npz', 'MRM_E240_like.npz', 'MRM_E240_like_82.npz', 'MRM_E240_like_82_2.npz', 'MRM_E240_neg.npz', 'MRM_E240_no25.npz', 'MRM_E240_no26.npz', 'MRM_E240_no33_1e-4.npz', 'MRM_E240_test.npz', 'MRM_LHO.npz']\n",
      "['MRM_E240_1e-4_5e-4_no33.npz', 'MRM_E240_1e-5_no20.npz', 'MRM_E240_like.npz', 'MRM_E240_like_82.npz', 'MRM_E240_like_82_2.npz', 'MRM_E240_neg.npz', 'MRM_E240_no25.npz', 'MRM_E240_no26.npz', 'MRM_E240_no33_1e-4.npz', 'MRM_E240_test.npz', 'MRM_LHO.npz']\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = './weight/LHO/'\n",
    "files = sorted(os.listdir(SAVE_DIR))\n",
    "print('files:', files)\n",
    "SAVE_NAMES = [string for string in files if string.startswith('MRM')]\n",
    "print(SAVE_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = './weight/LHO/'\n",
    "SAVE_NAMES = ['MRM_E240_no33_1e-4.npz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRM_E240_no33_1e-4.npz\n",
      "RS: (150, 165)\n",
      "(150, 82) (150, 82)\n",
      "num of positive data in testing: 1078.0\n",
      "(150, 82)\n",
      "\n",
      "==============================\n",
      "\n",
      "Top 1\n",
      "TP: 90\n",
      "prec: 0.6\n",
      "recall: 0.08348794063079777\n",
      "F1_score: 0.1465798045602606\n",
      "*****\n",
      "Top 3\n",
      "TP: 237\n",
      "prec: 0.5266666666666666\n",
      "recall: 0.21985157699443414\n",
      "F1_score: 0.3102094240837696\n",
      "*****\n",
      "Top 5\n",
      "TP: 346\n",
      "prec: 0.4613333333333333\n",
      "recall: 0.3209647495361781\n",
      "F1_score: 0.37855579868708966\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n",
      "NDCG@ 10\n",
      "NDCG score: 0.5231570195276019\n",
      "*****\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for SAVE_NAME in SAVE_NAMES:\n",
    "    RS = changes(SAVE_DIR, SAVE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample worst rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "new_test_f = []\n",
    "for i in range(len(test_idx)):\n",
    "    idx = test_idx[i]\n",
    "#     print(idx)\n",
    "    if not len(train_t[idx]+train_f[idx]+test_t[i]+test_f[i]) == movie_nb:\n",
    "        print('Error!!!')\n",
    "        break\n",
    "\n",
    "    a = set(np.argsort(RS[i])[:82])\n",
    "    neg = a & set(train_f[idx])\n",
    "#     print(len(neg))\n",
    "    choose = random.sample(neg, 50)\n",
    "    train_f[idx] = list(set(train_f[idx]) - set(choose))\n",
    "    new_test_f.append(test_f[i] + choose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_test_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_idx)):\n",
    "#     print(len(test_t[i]) + len(new_test_f[i]))\n",
    "    if not (len(test_t[i]) + len(new_test_f[i])) == 82:    \n",
    "        print('Error!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_idx)):\n",
    "    idx = test_idx[i]\n",
    "    if not len(train_t[idx]+train_f[idx]+test_t[i]+new_test_f[i]) == movie_nb:\n",
    "        print('Error!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './split/LHO_82_no26/'\n",
    "newPath(PATH)\n",
    "write_json(eval(str(train_t)), PATH + 'train_t.json')\n",
    "write_json(eval(str(train_f)), PATH + 'train_f.json')\n",
    "write_json(eval(str(test_t)), PATH + 'test_t.json')\n",
    "write_json(eval(str(test_f)), PATH + 'test_f_32.json')\n",
    "write_json(eval(str(new_test_f)), PATH + 'test_f_82.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
