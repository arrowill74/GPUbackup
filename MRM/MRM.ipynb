{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: (166, 300)\n",
      "Poster: (166, 256)\n",
      "IGimg: (166, 256)\n",
      "Movie genre: (166, 20)\n",
      "User following: (2020, 166)\n",
      "User genre: (2020, 20)\n"
     ]
    }
   ],
   "source": [
    "text_npy = np.load('./npy/text.npy')\n",
    "poster_npy = np.load('./npy/poster.npy')\n",
    "IGimg_npy = np.load('./npy/IGimg.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('Text:', text_npy.shape)\n",
    "print('Poster:', poster_npy.shape)\n",
    "print('IGimg:', IGimg_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [[2 1 0 ... 1 0 0]\n",
      " [0 0 0 ... 1 0 1]\n",
      " [3 7 4 ... 0 0 0]\n",
      " ...\n",
      " [5 3 0 ... 1 1 0]\n",
      " [2 2 0 ... 0 1 0]\n",
      " [2 1 0 ... 1 1 0]]\n",
      "After: [[0.22222222 0.11111111 0.         ... 0.11111111 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.125      0.         0.125     ]\n",
      " [0.33333333 0.77777778 0.44444444 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.26315789 0.15789474 0.         ... 0.05263158 0.05263158 0.        ]\n",
      " [0.28571429 0.28571429 0.         ... 0.         0.14285714 0.        ]\n",
      " [0.22222222 0.11111111 0.         ... 0.11111111 0.11111111 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print('Before:', usr_genre)\n",
    "print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 30\n"
     ]
    }
   ],
   "source": [
    "usr_test_amount = 200 # int(usr_npy.shape[0] * 0.1)\n",
    "movie_test_amount = 30 # int(movie_npy.shape[0] * 0.2)\n",
    "\n",
    "print(usr_test_amount, movie_test_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "test_idx = random.sample(usr_idx, usr_test_amount)\n",
    "print(len(test_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "train_t = [0]*(len(usr_following))\n",
    "train_f = [0]*(len(usr_following))\n",
    "# Testing \n",
    "test_t = [0]*usr_test_amount\n",
    "test_f = [0]*usr_test_amount\n",
    "test_pos = -1\n",
    "\n",
    "for i in range(len(usr_following)):\n",
    "    \n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(166):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train\n",
    "        \n",
    "    else: #if in test id, choose 2 true and other \n",
    "        test_pos += 1\n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(166):\n",
    "            \n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose 2 true and 8 false for test \n",
    "        t_for_test = random.sample(temp_t, 2)\n",
    "        f_for_test  = random.sample(temp_f, 8)\n",
    "        test_t[test_pos] = t_for_test\n",
    "        test_f[test_pos] = f_for_test\n",
    "        \n",
    "        #other for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t[i] = t_for_train\n",
    "        train_f[i] = f_for_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 2020\n",
      "The length of train_f: 2020\n",
      "The length of test_t: 200\n",
      "The length of test_f: 200\n"
     ]
    }
   ],
   "source": [
    "# train_t[i] 代表的是user i positive feedback\n",
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_t))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "laten_dim = 32 # latent dims\n",
    "ft_dim = 256 # feature dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user = tf.placeholder(tf.int32,[None])\n",
    "i = tf.placeholder(tf.int32, [None])\n",
    "j = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, [3, ft_dim])\n",
    "l_id = tf.placeholder(tf.int32, [3])\n",
    "r = tf.placeholder(tf.float32,[3])\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "image_j = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [usr_nb, laten_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [movie_nb, laten_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [movie_nb, laten_dim],\n",
    "                               initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    W1 = tf.get_variable(\"W1\", [usr_nb, laten_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [usr_nb,laten_dim,laten_dim], \n",
    "                         initializer=tf.contrib.layers.xavier_initializer()) #所有的user 都共用一個權重\n",
    "    Wa = tf.get_variable(\"Wa\", [usr_nb, laten_dim, laten_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wm = tf.get_variable(\"Wy\", [usr_nb, laten_dim, laten_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer()) #不同的電影有不同的權重\n",
    "    Wv = tf.get_variable(\"Wv\", [usr_nb, laten_dim, ft_dim],\n",
    "                         initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1, laten_dim], initializer=tf.constant_initializer(0.0))\n",
    "    ########## Error part, how to get auxisize dynamically\n",
    "    ####aux_size= tf.get_variable(name='aux_size', initializer=l_id.get_shape().as_list()[-1])\n",
    "    \n",
    "with tf.variable_scope('feature_level'):\n",
    "    Beta = tf.get_variable(\"beta\", [usr_nb, ft_dim],\n",
    "                           initializer=tf.random_normal_initializer(0.00001,0.000001,seed=10))\n",
    "\n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user) #(1*k) 第幾個user latent factor\n",
    "vi = tf.nn.embedding_lookup(item_latent, i)\n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "#取消 Weight 共用\n",
    "w1 = tf.nn.embedding_lookup(W1, user) #(1*k)\n",
    "wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user)) #(k*k)\n",
    "wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user)) #(k*k)\n",
    "wm = tf.squeeze(tf.nn.embedding_lookup(Wm, user)) #(k*k)\n",
    "wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user)) #(k,l)\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tonylab/miniconda3/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-4add3cf67128>:45: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "a_list=[]\n",
    "\n",
    "for q in range(3): #取l_id個 YouTuber 的 類別\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    a_list.append((tf.matmul( w1, tf.nn.relu( tf.matmul(wu, u, transpose_b=True) +\n",
    "        tf.matmul(wm, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wa, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "        tf.matmul(wv, xfi, transpose_b=True)))[0][0])*r[q])\n",
    "        \n",
    "                                          \n",
    "a_list_soft=tf.nn.softmax(a_list)\n",
    "#print(sess.run(a_list_soft))\n",
    "aux_np = np.zeros(laten_dim)\n",
    "\n",
    "#改成while\n",
    "for q in range(3): #取q個auxliary item\n",
    "    aux_np+=a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)\n",
    "\n",
    "aux_np+=u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new=tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,image_i, transpose_b=True)\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,image_j, transpose_b=True)\n",
    "\n",
    "xuij = xui- xuj\n",
    "\n",
    "l2_norm = tf.add_n([\n",
    "            0.001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.001 * tf.reduce_sum(tf.multiply(w1, w1)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wm, wm)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.001 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.1 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            \n",
    "          ])\n",
    "\n",
    "loss = l2_norm -tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraction: 0\n",
      "total_loss:----------------- [[0.56588656]]\n",
      "train_auc:------------------- 0.8188613861386138\n",
      "time: 270.62279629707336  sec\n",
      "Iteraction: 1\n",
      "total_loss:----------------- [[0.48105934]]\n",
      "train_auc:------------------- 0.7844306930693069\n",
      "time: 536.0553126335144  sec\n",
      "Iteraction: 2\n",
      "total_loss:----------------- [[0.4441793]]\n",
      "train_auc:------------------- 0.8003217821782178\n",
      "time: 805.3530004024506  sec\n",
      "Iteraction: 3\n",
      "total_loss:----------------- [[0.4165572]]\n",
      "train_auc:------------------- 0.8174009900990099\n",
      "time: 1077.22527718544  sec\n",
      "Iteraction: 4\n",
      "total_loss:----------------- [[0.40781003]]\n",
      "train_auc:------------------- 0.8221782178217821\n",
      "time: 1344.3898437023163  sec\n",
      "Iteraction: 5\n",
      "total_loss:----------------- [[0.39214453]]\n",
      "train_auc:------------------- 0.8309653465346535\n",
      "time: 1609.7548031806946  sec\n",
      "Iteraction: 6\n",
      "total_loss:----------------- [[0.38956276]]\n",
      "train_auc:------------------- 0.8328217821782178\n",
      "time: 1875.3597333431244  sec\n",
      "Iteraction: 7\n",
      "total_loss:----------------- [[0.39017385]]\n",
      "train_auc:------------------- 0.8355940594059406\n",
      "time: 2144.6734087467194  sec\n",
      "Iteraction: 8\n",
      "total_loss:----------------- [[0.38097584]]\n",
      "train_auc:------------------- 0.8402722772277228\n",
      "time: 2410.2362422943115  sec\n",
      "Iteraction: 9\n",
      "total_loss:----------------- [[0.37823504]]\n",
      "train_auc:------------------- 0.8421534653465347\n",
      "time: 2674.7854602336884  sec\n",
      "Total cost  2674.7857325077057  sec\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_pair_t = [] #positive feedback\n",
    "train_pair_f = [] #negative feedback\n",
    "train_yes_id = [] \n",
    "for q in range(10):\n",
    "    print('Iteraction:',q)\n",
    "    train_auc = 0\n",
    "    total_loss = 0\n",
    "    xuij_auc = 0\n",
    "    length = 0\n",
    "    \n",
    "    for z in range(usr_nb):\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes=[]\n",
    "        yesr=[]\n",
    "        r_3=np.zeros(3) \n",
    "        \n",
    "        #這裡不知道怎麼讓3變成變動型的長度\n",
    "        sample=random.sample(train_t[z],3) #隨機選3個sample true's YouTuber\n",
    "        train_yes_id.append(sample) #sample全部丟進去\n",
    "        \n",
    "        for k in range(len(sample)):\n",
    "            yes.append(IGimg_npy[sample[k]])\n",
    "            yesr.append(movie_genre[sample[k]]*usr_genre_norm[z])\n",
    "#             print('movie_genre:', movie_genre[sample[k]])\n",
    "#             print('usr_genre_norm:',usr_genre_norm[z])\n",
    "            \n",
    "        for k in range(3):\n",
    "            r_3[k]=max(yesr[k])\n",
    "        yes=np.array(yes)\n",
    "        \n",
    "        not_used_list = list(set(train_t[z]).difference(set(sample)))\n",
    "        \n",
    "        train_t_sample = random.sample(train_t[z],2)\n",
    "        #print('number of positive feedback', len(train_t[z]))\n",
    "        for ta in train_t_sample:\n",
    "            #ta=random.choice(train_t[z]) #ta is true positve photo\n",
    "            train_pair_t.append(ta)\n",
    "            image_1=np.expand_dims(IGimg_npy[ta],0)\n",
    "            \n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            for b in train_f_sample:\n",
    "                #print('likes:',ta,';Not likes:',b)\n",
    "                #b=random.choice(train_f[z])  #b is no feedback photo\n",
    "                train_pair_f.append(b)\n",
    "                image_2=np.expand_dims(IGimg_npy[b],0)\n",
    "                #print('Image_2',image_2.shape)\n",
    "            \n",
    "                #use_test[z].append(b)\n",
    "                r3,_auc, _loss,_=sess.run([a_list_soft,auc,loss,train_op], \n",
    "                                          feed_dict={user: [z], i: [ta], j: [b],\n",
    "                                                     xf: yes , l_id:sample,r:r_3,\n",
    "                                                     image_i:image_1,image_j:image_2})\n",
    "                #print(XUIJ)\n",
    "                #print('loss=',_loss)\n",
    "                #print('auc=',_auc)\n",
    "                #print('sub r3:',r3)\n",
    "                train_auc+=_auc\n",
    "                total_loss+=_loss\n",
    "                length += 1\n",
    "            #now1+=1\n",
    "    \n",
    "    #print('mine:',xuij_auc/136)    \n",
    "    #print('a_list_soft:',r3)\n",
    "    print(\"total_loss:-----------------\", total_loss/length)\n",
    "    print(\"train_auc:-------------------\", train_auc/length)\n",
    "    print('time:',time.time()-t0,' sec')\n",
    "print('Total cost ',time.time()-t0,' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, M, A, A1, Au, Am, Aa, Av,B = sess.run([user_latent, item_latent, aux_item, W1, Wu, Wm, Wa, Wv,Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent shape:  (2020, 32)\n",
      "photo latent shape:  (166, 32)\n",
      "Auxilary latent shape:  (166, 32)\n",
      "W1 weight shape:  (2020, 32)\n",
      "Wu weight shape: (2020, 32, 32)\n",
      "Wy weight shape: (2020, 32, 32)\n",
      "Wa weight shape: (2020, 32, 32)\n",
      "Wv weight shape: (2020, 32, 256)\n",
      "Beta shape: (2020, 256)\n"
     ]
    }
   ],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('W1 weight shape: ',A1.shape)\n",
    "print('Wu weight shape:',Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:',Aa.shape)\n",
    "print('Wv weight shape:',Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1613\n",
      "softmax alpha-------------- [0.09091353 0.09090975 0.09091393 0.09090153 0.09089638 0.09090909\n",
      " 0.09092979 0.09091765 0.0908948  0.09089801 0.09091555]\n",
      "1 976\n",
      "softmax alpha-------------- [0.09999805 0.10000009 0.09999888 0.09999762 0.10000073 0.0999969\n",
      " 0.10000392 0.10000327 0.09999952 0.10000101]\n",
      "2 1701\n",
      "softmax alpha-------------- [0.09999832 0.10000116 0.09999801 0.10000443 0.10000037 0.10000392\n",
      " 0.09999626 0.10000179 0.09999676 0.09999898]\n",
      "3 1579\n",
      "softmax alpha-------------- [0.11111152 0.11111348 0.1111069  0.11112669 0.11110689 0.11110431\n",
      " 0.11111232 0.11109956 0.11111832]\n",
      "4 437\n",
      "softmax alpha-------------- [0.07692758 0.07692198 0.0769213  0.07692369 0.07692378 0.07692322\n",
      " 0.07692234 0.0769224  0.0769235  0.07691917 0.07692347 0.07692073\n",
      " 0.07692683]\n",
      "5 1687\n",
      "softmax alpha-------------- [0.08332684 0.08332971 0.08333831 0.08333145 0.08333192 0.08333428\n",
      " 0.08333216 0.08333142 0.08333777 0.08333464 0.08333532 0.08333618]\n",
      "6 1760\n",
      "softmax alpha-------------- [0.11111169 0.11111781 0.11110719 0.11111371 0.1111155  0.11110496\n",
      " 0.11110985 0.11110541 0.11111387]\n",
      "7 416\n",
      "softmax alpha-------------- [0.07692607 0.0769223  0.07692439 0.07691671 0.07692356 0.07692621\n",
      " 0.07691804 0.07692544 0.07692716 0.07692388 0.07692501 0.07691629\n",
      " 0.07692492]\n",
      "8 1187\n",
      "softmax alpha-------------- [0.1428539  0.14285814 0.1428563  0.14285375 0.14285539 0.14286767\n",
      " 0.14285485]\n",
      "9 222\n",
      "softmax alpha-------------- [0.01190535 0.01190507 0.01190414 0.01190537 0.01190574 0.0119044\n",
      " 0.01190438 0.01190406 0.01190409 0.01190439 0.01190421 0.01190392\n",
      " 0.01190374 0.01190507 0.0119041  0.01190389 0.01190341 0.01190435\n",
      " 0.0119063  0.01190371 0.01190544 0.01190398 0.01190372 0.01190506\n",
      " 0.01190476 0.01190518 0.01190459 0.01190428 0.0119054  0.01190413\n",
      " 0.01190558 0.01190529 0.01190541 0.01190376 0.01190394 0.01190518\n",
      " 0.01190457 0.01190454 0.01190406 0.01190416 0.01190565 0.01190406\n",
      " 0.01190598 0.0119042  0.01190394 0.01190535 0.01190483 0.01190603\n",
      " 0.01190517 0.01190451 0.0119059  0.01190563 0.01190576 0.01190538\n",
      " 0.01190372 0.0119036  0.01190528 0.01190513 0.01190402 0.01190621\n",
      " 0.01190633 0.01190449 0.01190546 0.01190447 0.01190517 0.01190476\n",
      " 0.01190537 0.01190407 0.01190517 0.01190556 0.01190405 0.01190517\n",
      " 0.01190551 0.01190531 0.01190367 0.01190388 0.01190413 0.01190441\n",
      " 0.01190521 0.01190455 0.01190575 0.01190386 0.01190511 0.01190548]\n",
      "10 144\n",
      "softmax alpha-------------- [0.14286276 0.14285247 0.14285762 0.14285562 0.14285647 0.14285775\n",
      " 0.14285733]\n",
      "11 1732\n",
      "softmax alpha-------------- [0.14286429 0.14285903 0.14285112 0.1428552  0.14286617 0.14284712\n",
      " 0.14285706]\n",
      "12 1140\n",
      "softmax alpha-------------- [0.1        0.09999782 0.10000164 0.09999947 0.10000299 0.10000331\n",
      " 0.09999784 0.09999435 0.10000714 0.09999543]\n",
      "13 1665\n",
      "softmax alpha-------------- [0.11111594 0.11111742 0.11110473 0.11110843 0.11111733 0.11111766\n",
      " 0.1111117  0.11110004 0.11110675]\n",
      "14 970\n",
      "softmax alpha-------------- [0.03448427 0.03448421 0.03448451 0.03448082 0.03448317 0.03448313\n",
      " 0.03448368 0.0344821  0.03448402 0.03448256 0.03448113 0.03448464\n",
      " 0.03448272 0.03448313 0.03448156 0.03448116 0.03448266 0.03448231\n",
      " 0.03448514 0.03448243 0.03448017 0.03448212 0.03448253 0.03448458\n",
      " 0.03448343 0.0344838  0.03448179 0.03448178 0.03448044]\n",
      "15 1364\n",
      "softmax alpha-------------- [0.12499688 0.12499444 0.12501107 0.12500353 0.12500598 0.12499552\n",
      " 0.12499367 0.12499891]\n",
      "16 247\n",
      "softmax alpha-------------- [0.1250065  0.12499458 0.12500124 0.12501108 0.12498811 0.12498946\n",
      " 0.12499804 0.12501099]\n",
      "17 1656\n",
      "softmax alpha-------------- [0.12498902 0.12501205 0.12500891 0.12500109 0.12499064 0.12498459\n",
      " 0.1250102  0.12500349]\n",
      "18 472\n",
      "softmax alpha-------------- [0.1249951  0.12499528 0.12500208 0.12499095 0.12499454 0.1250099\n",
      " 0.12500272 0.12500942]\n",
      "19 889\n",
      "softmax alpha-------------- [0.11110895 0.11111203 0.11111131 0.11110779 0.11110994 0.11111061\n",
      " 0.11111195 0.11111599 0.11111143]\n",
      "20 801\n",
      "softmax alpha-------------- [0.07143298 0.07142737 0.07143041 0.07142525 0.0714308  0.0714309\n",
      " 0.07142847 0.07142563 0.0714256  0.07142663 0.0714265  0.07142994\n",
      " 0.07142944 0.07143008]\n",
      "21 577\n",
      "softmax alpha-------------- [0.0769251  0.07692329 0.07692432 0.07692437 0.07691955 0.07691949\n",
      " 0.07692009 0.07692434 0.07692434 0.07692298 0.07692311 0.07692413\n",
      " 0.07692487]\n",
      "22 1408\n",
      "softmax alpha-------------- [0.14286313 0.14286296 0.14285606 0.14285265 0.14285322 0.14284994\n",
      " 0.14286202]\n",
      "23 1054\n",
      "softmax alpha-------------- [0.11111728 0.11110755 0.11111688 0.11110435 0.11111164 0.11111175\n",
      " 0.11111037 0.11110894 0.11111124]\n",
      "24 864\n",
      "softmax alpha-------------- [0.1428535  0.14286494 0.14287054 0.1428685  0.14284385 0.14284931\n",
      " 0.14284935]\n",
      "25 1744\n",
      "softmax alpha-------------- [0.12499636 0.12500139 0.12499055 0.12498865 0.12501301 0.12499127\n",
      " 0.12500037 0.1250184 ]\n",
      "26 1899\n",
      "softmax alpha-------------- [0.12500884 0.12500007 0.12500855 0.12500141 0.12499874 0.12500471\n",
      " 0.12498328 0.12499441]\n",
      "27 1170\n",
      "softmax alpha-------------- [0.04762068 0.04762145 0.04762094 0.04761912 0.04761924 0.04761917\n",
      " 0.04761823 0.04761943 0.0476178  0.04761904 0.04761712 0.04761894\n",
      " 0.0476184  0.04761936 0.04761765 0.04761944 0.04761992 0.04761894\n",
      " 0.04761862 0.04761859 0.04761796]\n",
      "28 1336\n",
      "softmax alpha-------------- [0.14285949 0.14286322 0.14286604 0.14283834 0.14287009 0.14283805\n",
      " 0.14286477]\n",
      "29 27\n",
      "softmax alpha-------------- [0.12501069 0.12499084 0.12498994 0.12497475 0.12500738 0.12500647\n",
      " 0.1250127  0.12500723]\n",
      "30 420\n",
      "softmax alpha-------------- [0.09999378 0.09998331 0.10001025 0.10000886 0.09998571 0.09999911\n",
      " 0.10001129 0.10000939 0.10001408 0.09998422]\n",
      "31 24\n",
      "softmax alpha-------------- [0.14285704 0.14285593 0.14286704 0.14286391 0.1428544  0.14285217\n",
      " 0.14284951]\n",
      "32 1298\n",
      "softmax alpha-------------- [0.14285313 0.14286683 0.14285224 0.14285524 0.14286605 0.14285488\n",
      " 0.14285163]\n",
      "33 425\n",
      "softmax alpha-------------- [0.07692608 0.07692671 0.07692396 0.07691523 0.07692237 0.07692672\n",
      " 0.07692055 0.07692229 0.07691867 0.07692634 0.07691967 0.0769292\n",
      " 0.07692222]\n",
      "34 762\n",
      "softmax alpha-------------- [0.06249928 0.06249973 0.06250114 0.06250021 0.06250131 0.06249816\n",
      " 0.06249922 0.06250155 0.06250014 0.06250028 0.06249803 0.06249892\n",
      " 0.06250261 0.06249951 0.06249872 0.06250118]\n",
      "35 1641\n",
      "softmax alpha-------------- [0.12499939 0.1250047  0.1250062  0.12499834 0.12500405 0.12499792\n",
      " 0.12499637 0.12499304]\n",
      "36 1128\n",
      "softmax alpha-------------- [0.12500208 0.12500258 0.12499237 0.12500206 0.12500183 0.12499968\n",
      " 0.1249973  0.1250021 ]\n",
      "37 1250\n",
      "softmax alpha-------------- [0.02631593 0.0263192  0.02631837 0.02631542 0.0263154  0.02631443\n",
      " 0.02631819 0.02631619 0.02631665 0.0263182  0.02631481 0.02631341\n",
      " 0.02631559 0.02631419 0.02631721 0.02631545 0.02631573 0.02631757\n",
      " 0.02631733 0.02631366 0.02631646 0.02631389 0.0263143  0.02631626\n",
      " 0.02631857 0.026314   0.02631455 0.02631474 0.02631451 0.02631565\n",
      " 0.02631637 0.02631546 0.02631349 0.02631583 0.02631496 0.02631658\n",
      " 0.02631619 0.02631523]\n",
      "38 1259\n",
      "softmax alpha-------------- [0.14283935 0.14287406 0.14285284 0.14287009 0.14284575 0.14286103\n",
      " 0.14285687]\n",
      "39 1638\n",
      "softmax alpha-------------- [0.14284923 0.14285998 0.14285819 0.14286307 0.14285839 0.14285892\n",
      " 0.14285222]\n",
      "40 1083\n",
      "softmax alpha-------------- [0.09999677 0.09999475 0.10000239 0.09999593 0.09999785 0.10000095\n",
      " 0.10000011 0.10000894 0.10000323 0.09999908]\n",
      "41 1389\n",
      "softmax alpha-------------- [0.06666942 0.06666071 0.06666393 0.06666917 0.06666853 0.06666214\n",
      " 0.06666963 0.06666569 0.06667187 0.06666911 0.06667221 0.06666125\n",
      " 0.06666358 0.0666696  0.06666316]\n",
      "42 1727\n",
      "softmax alpha-------------- [0.0370381  0.03703612 0.03703622 0.03703575 0.03703674 0.03703673\n",
      " 0.0370389  0.03703587 0.03703807 0.03703648 0.03704079 0.03703679\n",
      " 0.03703687 0.03704063 0.03703594 0.03703513 0.037036   0.03703583\n",
      " 0.03703868 0.03703554 0.03703651 0.03703795 0.03703609 0.03703746\n",
      " 0.03703689 0.03703559 0.0370383 ]\n",
      "43 1662\n",
      "softmax alpha-------------- [0.14285847 0.14286464 0.14285671 0.14285486 0.14285436 0.14285842\n",
      " 0.14285254]\n",
      "44 153\n",
      "softmax alpha-------------- [0.11112477 0.11112777 0.11112411 0.11108101 0.11108696 0.11112706\n",
      " 0.11112167 0.11111008 0.11109655]\n",
      "45 2019\n",
      "softmax alpha-------------- [0.11111121 0.11111102 0.11111085 0.11111143 0.11111092 0.11111124\n",
      " 0.11111097 0.11111148 0.11111088]\n",
      "46 457\n",
      "softmax alpha-------------- [0.09090589 0.09090916 0.09090666 0.09091258 0.09089943 0.09090282\n",
      " 0.09092575 0.09089946 0.09091055 0.09090218 0.09092552]\n",
      "47 932\n",
      "softmax alpha-------------- [0.03124719 0.0312466  0.03125023 0.03124782 0.03125064 0.03125082\n",
      " 0.03124931 0.03125359 0.03125006 0.03124749 0.03125544 0.03124785\n",
      " 0.03125471 0.0312527  0.03125418 0.0312528  0.03125006 0.03125392\n",
      " 0.03124702 0.03124875 0.0312487  0.03124689 0.03124692 0.03125014\n",
      " 0.03124775 0.03124616 0.0312469  0.0312518  0.0312544  0.03125392\n",
      " 0.03124902 0.03124622]\n",
      "48 1974\n",
      "softmax alpha-------------- [0.04761828 0.04762057 0.04761866 0.04761996 0.04762004 0.04761865\n",
      " 0.04761854 0.04761915 0.04761918 0.04761937 0.04761738 0.04761886\n",
      " 0.04761856 0.04761983 0.04761956 0.04761989 0.04761891 0.04761714\n",
      " 0.0476219  0.047619   0.04761656]\n",
      "49 93\n",
      "softmax alpha-------------- [0.0909057  0.09090488 0.09090252 0.09090976 0.09090096 0.09092155\n",
      " 0.09090145 0.09092291 0.09092002 0.09091727 0.09089299]\n",
      "50 561\n",
      "softmax alpha-------------- [0.11111386 0.11111537 0.11110692 0.11111263 0.11110634 0.11111297\n",
      " 0.11111081 0.11111172 0.11110937]\n",
      "51 1323\n",
      "softmax alpha-------------- [0.14284273 0.14283858 0.14286186 0.14286172 0.14287498 0.14287512\n",
      " 0.14284501]\n",
      "52 1872\n",
      "softmax alpha-------------- [0.14286353 0.1428496  0.1428535  0.14285897 0.14286216 0.14285284\n",
      " 0.1428594 ]\n",
      "53 1422\n",
      "softmax alpha-------------- [0.0769251  0.07691838 0.07691696 0.07692335 0.07691638 0.07692497\n",
      " 0.07693003 0.07692604 0.0769253  0.0769275  0.07691714 0.07692505\n",
      " 0.07692381]\n",
      "54 1589\n",
      "softmax alpha-------------- [0.14286739 0.14285577 0.14286037 0.14285006 0.14285539 0.14285671\n",
      " 0.1428543 ]\n",
      "55 453\n",
      "softmax alpha-------------- [0.14285382 0.14286344 0.14285485 0.14284704 0.14284474 0.14287414\n",
      " 0.14286197]\n",
      "56 822\n",
      "softmax alpha-------------- [0.12500796 0.12499106 0.12500443 0.12499915 0.12499526 0.12501723\n",
      " 0.12499658 0.12498833]\n",
      "57 1484\n",
      "softmax alpha-------------- [0.05263105 0.05263156 0.05263636 0.05263021 0.05263208 0.05262686\n",
      " 0.05263342 0.05263031 0.05263162 0.05263512 0.0526313  0.0526312\n",
      " 0.05263279 0.05262965 0.05262886 0.05262739 0.05263113 0.05263907\n",
      " 0.05263002]\n",
      "58 499\n",
      "softmax alpha-------------- [0.05263016 0.05262319 0.05263541 0.05262789 0.05263467 0.05263473\n",
      " 0.05263139 0.05262926 0.05263518 0.05263256 0.05263739 0.05263427\n",
      " 0.05262944 0.05263395 0.05262782 0.05262325 0.05263203 0.05263342\n",
      " 0.052634  ]\n",
      "59 956\n",
      "softmax alpha-------------- [0.12500067 0.1250043  0.12500509 0.12499357 0.12499758 0.1249955\n",
      " 0.12499908 0.12500422]\n",
      "60 1830\n",
      "softmax alpha-------------- [0.11111624 0.1111115  0.11110904 0.11110992 0.11110597 0.11111311\n",
      " 0.1111088  0.11111159 0.11111383]\n",
      "61 423\n",
      "softmax alpha-------------- [0.14285543 0.14286132 0.14285542 0.14285493 0.14285975 0.1428562\n",
      " 0.14285697]\n",
      "62 1361\n",
      "softmax alpha-------------- [0.14286528 0.14285542 0.14285574 0.14284708 0.1428503  0.14286583\n",
      " 0.14286035]\n",
      "63 686\n",
      "softmax alpha-------------- [0.12499354 0.12499675 0.12500655 0.1250052  0.12499883 0.12499966\n",
      " 0.12500351 0.12499596]\n",
      "64 157\n",
      "softmax alpha-------------- [0.14285182 0.1428576  0.14285358 0.14286291 0.14284853 0.14286316\n",
      " 0.14286239]\n",
      "65 1145\n",
      "softmax alpha-------------- [0.11110886 0.1111075  0.11110783 0.11110567 0.11111395 0.11111536\n",
      " 0.11112007 0.11110411 0.11111666]\n",
      "66 198\n",
      "softmax alpha-------------- [0.14285565 0.14285905 0.14286639 0.14286415 0.14284233 0.14285966\n",
      " 0.14285277]\n",
      "67 1625\n",
      "softmax alpha-------------- [0.14285918 0.1428641  0.14285538 0.14285084 0.14285358 0.1428576\n",
      " 0.14285932]\n",
      "68 733\n",
      "softmax alpha-------------- [0.14285117 0.14286207 0.14286044 0.14285588 0.14285673 0.14286074\n",
      " 0.14285298]\n",
      "69 1097\n",
      "softmax alpha-------------- [0.05262727 0.05263057 0.05263238 0.05263084 0.0526346  0.05262966\n",
      " 0.05264139 0.05262743 0.05263267 0.05263313 0.05262727 0.05263382\n",
      " 0.05263023 0.05263286 0.0526315  0.05262798 0.05263417 0.05263108\n",
      " 0.05263113]\n",
      "70 836\n",
      "softmax alpha-------------- [0.14285939 0.1428524  0.14286015 0.14285838 0.14285513 0.14285625\n",
      " 0.14285829]\n",
      "71 523\n",
      "softmax alpha-------------- [0.12499941 0.12500031 0.12499814 0.12500457 0.12499952 0.12499632\n",
      " 0.12500364 0.12499808]\n",
      "72 164\n",
      "softmax alpha-------------- [0.09999787 0.09999387 0.10000474 0.10000425 0.09999294 0.10000149\n",
      " 0.100003   0.09999756 0.10000202 0.10000225]\n",
      "73 1746\n",
      "softmax alpha-------------- [0.06666779 0.06666361 0.06667407 0.06666003 0.06667083 0.06666501\n",
      " 0.06666909 0.06667224 0.06665854 0.06667497 0.06666001 0.06666931\n",
      " 0.0666594  0.06666966 0.06666545]\n",
      "74 4\n",
      "softmax alpha-------------- [0.14285506 0.14285062 0.14285475 0.14285551 0.14285449 0.14285608\n",
      " 0.14287348]\n",
      "75 441\n",
      "softmax alpha-------------- [0.1428492  0.14285701 0.14285476 0.14285098 0.1428614  0.14285722\n",
      " 0.14286942]\n",
      "76 629\n",
      "softmax alpha-------------- [0.09999778 0.09999899 0.09999885 0.09999506 0.10000107 0.09999879\n",
      " 0.10000278 0.09999971 0.10000348 0.1000035 ]\n",
      "77 391\n",
      "softmax alpha-------------- [0.12501246 0.1249983  0.1249988  0.12500068 0.12499628 0.12499791\n",
      " 0.12499935 0.12499622]\n",
      "78 1985\n",
      "softmax alpha-------------- [0.07142664 0.0714301  0.07143285 0.07143327 0.07142847 0.07142499\n",
      " 0.07142739 0.07143081 0.0714283  0.07142795 0.07142601 0.07143093\n",
      " 0.07142528 0.07142704]\n",
      "79 1906\n",
      "softmax alpha-------------- [0.11111809 0.11110853 0.11111647 0.11110585 0.11110834 0.11110898\n",
      " 0.11110795 0.11110597 0.11111981]\n",
      "80 1127\n",
      "softmax alpha-------------- [0.09090252 0.09090683 0.09090478 0.09091714 0.09092185 0.09090737\n",
      " 0.09091614 0.09090517 0.09090758 0.09090847 0.09090215]\n",
      "81 1966\n",
      "softmax alpha-------------- [0.08334358 0.08333467 0.08332028 0.08333202 0.08334338 0.08334245\n",
      " 0.08331685 0.08334059 0.08333526 0.08332479 0.08333077 0.08333534]\n",
      "82 1639\n",
      "softmax alpha-------------- [0.12499869 0.1249975  0.12500626 0.12500172 0.12499585 0.12499874\n",
      " 0.12499829 0.12500296]\n",
      "83 597\n",
      "softmax alpha-------------- [0.09999697 0.09999791 0.09999908 0.1000011  0.10000526 0.1000004\n",
      " 0.10000089 0.1000007  0.10000066 0.09999704]\n",
      "84 1418\n",
      "softmax alpha-------------- [0.04762079 0.04762322 0.04761418 0.04761882 0.04761655 0.04762338\n",
      " 0.04761936 0.0476199  0.04761817 0.04761664 0.04762026 0.04761981\n",
      " 0.04761541 0.04761596 0.04761533 0.0476229  0.04761978 0.04761739\n",
      " 0.04762353 0.04761869 0.04761992]\n",
      "85 1434\n",
      "softmax alpha-------------- [0.12501402 0.12501228 0.12499695 0.12499101 0.12499446 0.12499147\n",
      " 0.12500352 0.12499629]\n",
      "86 122\n",
      "softmax alpha-------------- [0.0833348  0.08333718 0.08332733 0.08333108 0.08333433 0.08333393\n",
      " 0.08333504 0.08333153 0.08333582 0.08333143 0.08333192 0.08333562]\n",
      "87 1752\n",
      "softmax alpha-------------- [0.12500092 0.12500086 0.12500036 0.12500525 0.12500071 0.12498374\n",
      " 0.12500579 0.12500237]\n",
      "88 750\n",
      "softmax alpha-------------- [0.11111415 0.11111263 0.11110846 0.11111087 0.11111304 0.11110947\n",
      " 0.11111386 0.1111109  0.11110662]\n",
      "89 1148\n",
      "softmax alpha-------------- [0.09999709 0.10000278 0.09999971 0.0999964  0.09999967 0.09999959\n",
      " 0.10000597 0.10000069 0.09999741 0.10000069]\n",
      "90 1398\n",
      "softmax alpha-------------- [0.14285091 0.14284851 0.14284931 0.14285262 0.14285951 0.14287371\n",
      " 0.14286544]\n",
      "91 770\n",
      "softmax alpha-------------- [0.14285413 0.14286166 0.14286028 0.14285778 0.14285166 0.14285801\n",
      " 0.14285647]\n",
      "92 1048\n",
      "softmax alpha-------------- [0.1250061  0.12499536 0.12500491 0.12500467 0.12500208 0.12498985\n",
      " 0.12499363 0.1250034 ]\n",
      "93 817\n",
      "softmax alpha-------------- [0.12499582 0.12500466 0.12499104 0.1250031  0.12499694 0.12500853\n",
      " 0.12500358 0.12499633]\n",
      "94 1689\n",
      "softmax alpha-------------- [0.09090525 0.09090744 0.09091465 0.0909083  0.09091158 0.09090453\n",
      " 0.09090902 0.09091075 0.09090789 0.09091265 0.09090794]\n",
      "95 290\n",
      "softmax alpha-------------- [0.07143184 0.07143235 0.07142792 0.07142519 0.07142639 0.07143139\n",
      " 0.07142325 0.07143283 0.07142292 0.07142312 0.07143069 0.07142826\n",
      " 0.07143081 0.07143304]\n",
      "96 1217\n",
      "softmax alpha-------------- [0.12500202 0.12499701 0.12500173 0.12500347 0.12499768 0.12499856\n",
      " 0.12500195 0.12499759]\n",
      "97 1603\n",
      "softmax alpha-------------- [0.14287319 0.14285925 0.142845   0.1428526  0.14285124 0.14286524\n",
      " 0.14285349]\n",
      "98 1771\n",
      "softmax alpha-------------- [0.06250007 0.06250063 0.06249507 0.06250288 0.06250311 0.06250125\n",
      " 0.0625025  0.06249689 0.06250159 0.06250049 0.06250267 0.06249947\n",
      " 0.0624988  0.06250295 0.06249228 0.06249935]\n",
      "99 1948\n",
      "softmax alpha-------------- [0.11110332 0.11111842 0.11111003 0.11111774 0.11110245 0.11111088\n",
      " 0.11111182 0.11111882 0.11110652]\n",
      "100 742\n",
      "softmax alpha-------------- [0.12500208 0.12499167 0.12500597 0.1249971  0.12499052 0.12500403\n",
      " 0.12501111 0.12499752]\n",
      "101 1870\n",
      "softmax alpha-------------- [0.12499822 0.12499423 0.12500656 0.12499675 0.12499748 0.1250118\n",
      " 0.1249987  0.12499625]\n",
      "102 404\n",
      "softmax alpha-------------- [0.14284843 0.14284784 0.14287114 0.14285618 0.14285844 0.14285287\n",
      " 0.14286511]\n",
      "103 314\n",
      "softmax alpha-------------- [0.11110978 0.11110853 0.11111498 0.11110608 0.11111557 0.11111612\n",
      " 0.11110524 0.11110916 0.11111453]\n",
      "104 934\n",
      "softmax alpha-------------- [0.01754468 0.01754422 0.01754542 0.01754346 0.01754315 0.01754359\n",
      " 0.01754361 0.01754384 0.01754344 0.01754313 0.01754414 0.01754333\n",
      " 0.01754282 0.01754474 0.01754322 0.01754477 0.01754483 0.01754386\n",
      " 0.0175437  0.01754437 0.01754413 0.01754356 0.01754269 0.01754371\n",
      " 0.01754339 0.01754425 0.01754408 0.01754398 0.01754446 0.01754452\n",
      " 0.01754342 0.01754395 0.01754366 0.01754342 0.01754396 0.01754315\n",
      " 0.01754434 0.01754377 0.01754407 0.01754345 0.01754404 0.01754451\n",
      " 0.0175437  0.01754545 0.01754316 0.01754344 0.01754427 0.01754417\n",
      " 0.01754315 0.01754321 0.01754417 0.01754314 0.01754451 0.01754308\n",
      " 0.01754334 0.01754475 0.01754365]\n",
      "105 1608\n",
      "softmax alpha-------------- [0.12501364 0.12499943 0.12499843 0.12500006 0.12499181 0.12499313\n",
      " 0.12500123 0.12500227]\n",
      "106 1006\n",
      "softmax alpha-------------- [0.14285471 0.1428559  0.14285535 0.14285471 0.14285772 0.14285621\n",
      " 0.14286541]\n",
      "107 530\n",
      "softmax alpha-------------- [0.14287806 0.14285827 0.14284951 0.14285916 0.14285959 0.14284927\n",
      " 0.14284614]\n",
      "108 1022\n",
      "softmax alpha-------------- [0.03448227 0.03448278 0.03448269 0.03448293 0.03448416 0.0344824\n",
      " 0.03448219 0.03448286 0.0344833  0.03448294 0.03448413 0.0344824\n",
      " 0.03448254 0.03448234 0.0344813  0.03448286 0.03448392 0.03448182\n",
      " 0.0344808  0.03448319 0.03448314 0.03448169 0.03448244 0.03448343\n",
      " 0.03448416 0.03448228 0.03448285 0.03448328 0.03448291]\n",
      "109 706\n",
      "softmax alpha-------------- [0.14284191 0.14286082 0.14286464 0.14286555 0.14287382 0.14284684\n",
      " 0.14284642]\n",
      "110 371\n",
      "softmax alpha-------------- [0.04347799 0.04347763 0.04347849 0.04347743 0.04347708 0.04347851\n",
      " 0.0434794  0.04347622 0.04347812 0.04348176 0.04347703 0.04347674\n",
      " 0.04348094 0.04347969 0.04347849 0.0434796  0.04347904 0.04348009\n",
      " 0.04347606 0.04347732 0.04347975 0.04347597 0.04347665]\n",
      "111 451\n",
      "softmax alpha-------------- [0.02439064 0.02439021 0.02438914 0.02439033 0.02438932 0.02439061\n",
      " 0.02438866 0.0243892  0.02439203 0.024389   0.0243918  0.02438903\n",
      " 0.02439012 0.02439174 0.02438906 0.02438923 0.02439281 0.02439146\n",
      " 0.02438889 0.02439045 0.02438976 0.02438768 0.02439005 0.02439094\n",
      " 0.02438955 0.02438903 0.02439181 0.02438853 0.02439231 0.02438942\n",
      " 0.02439144 0.02438972 0.02439145 0.02438933 0.02439105 0.02439091\n",
      " 0.0243912  0.02438952 0.02439108 0.02439065 0.02439081]\n",
      "112 1865\n",
      "softmax alpha-------------- [0.12500021 0.12499406 0.12500037 0.12502109 0.12499365 0.12499763\n",
      " 0.12499526 0.12499773]\n",
      "113 66\n",
      "softmax alpha-------------- [0.14285904 0.14286062 0.14285744 0.14285288 0.14285684 0.14285734\n",
      " 0.14285584]\n",
      "114 780\n",
      "softmax alpha-------------- [0.11110393 0.11111256 0.11111491 0.11110892 0.11111027 0.11111816\n",
      " 0.11111312 0.11111245 0.11110569]\n",
      "115 1529\n",
      "softmax alpha-------------- [0.12499259 0.12499861 0.12500531 0.12497841 0.12500368 0.12500702\n",
      " 0.12500891 0.12500547]\n",
      "116 1922\n",
      "softmax alpha-------------- [0.05263317 0.05263087 0.05263217 0.05263223 0.0526317  0.05263137\n",
      " 0.05263364 0.05263145 0.05263046 0.05262914 0.05263137 0.05263128\n",
      " 0.05263346 0.05262884 0.05263279 0.0526283  0.05263199 0.05263213\n",
      " 0.05263362]\n",
      "117 1986\n",
      "softmax alpha-------------- [0.09999757 0.09999782 0.10000707 0.10000001 0.09998627 0.1000031\n",
      " 0.0999978  0.10000713 0.1000002  0.10000304]\n",
      "118 1243\n",
      "softmax alpha-------------- [0.12500768 0.12500375 0.12499738 0.12500323 0.12499128 0.12498922\n",
      " 0.12500192 0.12500554]\n",
      "119 1285\n",
      "softmax alpha-------------- [0.08332519 0.08334472 0.08333035 0.0833375  0.08333383 0.08332744\n",
      " 0.08332674 0.08333893 0.08334031 0.08333619 0.08332795 0.08333085]\n",
      "120 184\n",
      "softmax alpha-------------- [0.14285579 0.14286418 0.14284638 0.14285123 0.14287206 0.142861\n",
      " 0.14284935]\n",
      "121 1663\n",
      "softmax alpha-------------- [0.12499514 0.12500365 0.12499937 0.12500914 0.12500717 0.12499662\n",
      " 0.12499184 0.12499707]\n",
      "122 285\n",
      "softmax alpha-------------- [0.12501126 0.12501643 0.12499661 0.12498937 0.12499588 0.12499443\n",
      " 0.12500108 0.12499494]\n",
      "123 1588\n",
      "softmax alpha-------------- [0.11111143 0.11110709 0.11111205 0.11110732 0.11111212 0.11111349\n",
      " 0.11111172 0.11111424 0.11111055]\n",
      "124 544\n",
      "softmax alpha-------------- [0.0909067  0.09090494 0.09090131 0.09091143 0.09091246 0.09090905\n",
      " 0.09090951 0.0909086  0.09091171 0.0909228  0.09090146]\n",
      "125 1291\n",
      "softmax alpha-------------- [0.0833321  0.08333295 0.0833356  0.08333195 0.08333216 0.08333575\n",
      " 0.08332985 0.08333731 0.08333181 0.08333329 0.08333542 0.0833318 ]\n",
      "126 940\n",
      "softmax alpha-------------- [0.03571374 0.03571312 0.03571466 0.03571468 0.03571406 0.03571305\n",
      " 0.03571411 0.03571361 0.03571583 0.03571368 0.0357137  0.03571297\n",
      " 0.03571555 0.03571409 0.0357142  0.035713   0.03571684 0.03571328\n",
      " 0.03571764 0.03571245 0.03571436 0.03571338 0.03571503 0.03571561\n",
      " 0.03571306 0.03571409 0.03571555 0.03571466]\n",
      "127 1694\n",
      "softmax alpha-------------- [0.02500092 0.02499967 0.02499857 0.02500009 0.02500151 0.02499961\n",
      " 0.02499952 0.02500083 0.02499858 0.02500002 0.02500089 0.02499932\n",
      " 0.0249992  0.02499996 0.02499924 0.02499995 0.02499938 0.02499978\n",
      " 0.02500094 0.0250004  0.02500029 0.02500062 0.02500097 0.0249998\n",
      " 0.02500156 0.0249998  0.02500064 0.02499951 0.02499895 0.02499948\n",
      " 0.02499906 0.02499971 0.02499991 0.02499931 0.02500066 0.02500025\n",
      " 0.0250002  0.0249999  0.02500066 0.02500035]\n",
      "128 1184\n",
      "softmax alpha-------------- [0.07692352 0.076923   0.07692137 0.07691524 0.07692192 0.07692334\n",
      " 0.07692742 0.07692795 0.07692109 0.07691976 0.07692678 0.07692226\n",
      " 0.07692636]\n",
      "129 1206\n",
      "softmax alpha-------------- [0.1428619  0.14285531 0.14285862 0.14285462 0.14285418 0.14285455\n",
      " 0.14286082]\n",
      "130 778\n",
      "softmax alpha-------------- [0.05555502 0.05555609 0.05555472 0.05555739 0.0555558  0.05555447\n",
      " 0.05555523 0.05555564 0.0555556  0.05555567 0.05555525 0.05555895\n",
      " 0.05555482 0.05555355 0.05555843 0.05555304 0.05555451 0.05555585]\n",
      "131 470\n",
      "softmax alpha-------------- [0.12499707 0.12499301 0.12499221 0.12499936 0.12501345 0.12499504\n",
      " 0.12500421 0.12500564]\n",
      "132 430\n",
      "softmax alpha-------------- [0.09091546 0.09090897 0.09090389 0.09090315 0.090913   0.09091177\n",
      " 0.09091177 0.09090338 0.09091028 0.090911   0.09090734]\n",
      "133 412\n",
      "softmax alpha-------------- [0.14286453 0.14286914 0.14287904 0.14284073 0.14284466 0.14286588\n",
      " 0.14283602]\n",
      "134 735\n",
      "softmax alpha-------------- [0.14285948 0.14285695 0.14285512 0.14285659 0.14285773 0.14285597\n",
      " 0.14285815]\n",
      "135 1125\n",
      "softmax alpha-------------- [0.11110845 0.11111655 0.11111481 0.11109538 0.11112086 0.11112449\n",
      " 0.11110637 0.11111855 0.11109453]\n",
      "136 1814\n",
      "softmax alpha-------------- [0.06666472 0.06666056 0.06666723 0.06666431 0.0666705  0.06666385\n",
      " 0.06666545 0.06666676 0.06666241 0.066667   0.06666478 0.06666115\n",
      " 0.06667643 0.06667363 0.06667125]\n",
      "137 758\n",
      "softmax alpha-------------- [0.12499992 0.12499978 0.12499149 0.12499212 0.12500621 0.12500575\n",
      " 0.12499854 0.1250062 ]\n",
      "138 1292\n",
      "softmax alpha-------------- [0.14285479 0.14285593 0.14285797 0.14286427 0.1428521  0.14285469\n",
      " 0.14286025]\n",
      "139 1105\n",
      "softmax alpha-------------- [0.14285656 0.14286497 0.14286327 0.14285005 0.1428461  0.14284974\n",
      " 0.14286931]\n",
      "140 1237\n",
      "softmax alpha-------------- [0.14285327 0.14285714 0.14285327 0.1428604  0.14285758 0.14285894\n",
      " 0.1428594 ]\n",
      "141 1313\n",
      "softmax alpha-------------- [0.05000195 0.05000212 0.04999818 0.04999665 0.05000087 0.0500014\n",
      " 0.05000088 0.0499968  0.05000121 0.0500018  0.05000248 0.04999949\n",
      " 0.04999974 0.05000156 0.04999801 0.04999449 0.05000125 0.04999812\n",
      " 0.0500022  0.05000078]\n",
      "142 1064\n",
      "softmax alpha-------------- [0.09999937 0.10000082 0.09999676 0.09999971 0.09999669 0.10000451\n",
      " 0.09999516 0.10000123 0.10000167 0.10000408]\n",
      "143 225\n",
      "softmax alpha-------------- [0.07143011 0.07142835 0.07142636 0.07142837 0.07142957 0.07142785\n",
      " 0.07142896 0.07142777 0.07142777 0.07142702 0.07142862 0.07142801\n",
      " 0.07143191 0.07142933]\n",
      "144 1959\n",
      "softmax alpha-------------- [0.05000403 0.05000385 0.05000158 0.04999553 0.05000343 0.04999781\n",
      " 0.04999871 0.05000033 0.0499999  0.04999957 0.0500001  0.04999923\n",
      " 0.05000109 0.04999844 0.04999986 0.04999825 0.05000064 0.04999614\n",
      " 0.05000218 0.04999931]\n",
      "145 1139\n",
      "softmax alpha-------------- [0.07692109 0.07691774 0.07692288 0.07693069 0.07691933 0.07692174\n",
      " 0.07691917 0.0769253  0.07692117 0.07692646 0.07693256 0.07692045\n",
      " 0.07692142]\n",
      "146 1705\n",
      "softmax alpha-------------- [0.03448383 0.03448293 0.0344832  0.03448325 0.03448185 0.03448179\n",
      " 0.03448113 0.03448472 0.03448278 0.03448128 0.03448138 0.03448336\n",
      " 0.03448353 0.03448338 0.03448346 0.03448153 0.03448215 0.03448286\n",
      " 0.03448238 0.03448297 0.03448379 0.03448253 0.03448293 0.03448289\n",
      " 0.03448242 0.03448242 0.03448346 0.03448179 0.03448399]\n",
      "147 979\n",
      "softmax alpha-------------- [0.08334131 0.08333688 0.08332883 0.08332067 0.08333946 0.08332661\n",
      " 0.08334049 0.08333804 0.08331082 0.08333265 0.08334179 0.08334246]\n",
      "148 33\n",
      "softmax alpha-------------- [0.11111127 0.11110664 0.11111231 0.11111062 0.11111257 0.11111578\n",
      " 0.1111106  0.11110565 0.11111456]\n",
      "149 273\n",
      "softmax alpha-------------- [0.14287079 0.14285908 0.1428694  0.1428443  0.14285656 0.14284471\n",
      " 0.14285515]\n",
      "150 327\n",
      "softmax alpha-------------- [0.12501376 0.1250037  0.12500912 0.12500566 0.12499891 0.1249993\n",
      " 0.12499306 0.12497649]\n",
      "151 1178\n",
      "softmax alpha-------------- [0.11111207 0.11111708 0.11110642 0.11111119 0.11110756 0.11111061\n",
      " 0.11111033 0.11111136 0.11111337]\n",
      "152 201\n",
      "softmax alpha-------------- [0.12500117 0.12499866 0.12500185 0.12499688 0.12500218 0.12499509\n",
      " 0.12500532 0.12499885]\n",
      "153 1073\n",
      "softmax alpha-------------- [0.14285441 0.14285063 0.14286218 0.14285812 0.14285669 0.14285587\n",
      " 0.14286209]\n",
      "154 270\n",
      "softmax alpha-------------- [0.14286057 0.14285835 0.14285966 0.14286419 0.14285108 0.14284971\n",
      " 0.14285644]\n",
      "155 220\n",
      "softmax alpha-------------- [0.14285739 0.1428613  0.14285786 0.14285554 0.14285809 0.14285446\n",
      " 0.14285537]\n",
      "156 1348\n",
      "softmax alpha-------------- [0.14284515 0.1428706  0.14284078 0.14287263 0.14285793 0.14283861\n",
      " 0.14287431]\n",
      "157 518\n",
      "softmax alpha-------------- [0.10000207 0.10000398 0.10000787 0.10000041 0.09999377 0.10000452\n",
      " 0.10000366 0.09998915 0.10000313 0.09999143]\n",
      "158 233\n",
      "softmax alpha-------------- [0.11111497 0.11110192 0.11110783 0.11112114 0.11111809 0.11110494\n",
      " 0.11110801 0.11110733 0.11111577]\n",
      "159 1915\n",
      "softmax alpha-------------- [0.04166346 0.04166896 0.04166828 0.04166893 0.04166966 0.04166819\n",
      " 0.04166625 0.04166503 0.04166873 0.04166877 0.04166601 0.04166311\n",
      " 0.04166765 0.04166654 0.04166585 0.0416662  0.0416695  0.04166365\n",
      " 0.04166778 0.04166373 0.0416667  0.0416645  0.04166464 0.04166789]\n",
      "160 849\n",
      "softmax alpha-------------- [0.09091402 0.09091687 0.09091266 0.09090695 0.09090687 0.09091086\n",
      " 0.09090326 0.09090767 0.09090355 0.09091023 0.09090706]\n",
      "161 154\n",
      "softmax alpha-------------- [0.09090898 0.09092122 0.09091167 0.09090673 0.09091002 0.09090696\n",
      " 0.09090762 0.09090901 0.0909076  0.09090392 0.09090628]\n",
      "162 652\n",
      "softmax alpha-------------- [0.04545563 0.04545797 0.04545151 0.04545269 0.04545147 0.04545892\n",
      " 0.0454502  0.04545219 0.04544957 0.04545852 0.0454536  0.04545377\n",
      " 0.04545825 0.04545292 0.04545211 0.04545898 0.04545254 0.04545537\n",
      " 0.04545839 0.04545505 0.04545096 0.04545939]\n",
      "163 907\n",
      "softmax alpha-------------- [0.10000247 0.09998997 0.10000943 0.09999472 0.09999355 0.10000071\n",
      " 0.09999498 0.10000683 0.10000985 0.09999748]\n",
      "164 104\n",
      "softmax alpha-------------- [0.14285906 0.14285251 0.14285936 0.14286296 0.14285031 0.14286306\n",
      " 0.14285275]\n",
      "165 1559\n",
      "softmax alpha-------------- [0.10000695 0.09999422 0.09999611 0.09999608 0.10000168 0.1000018\n",
      " 0.09998962 0.0999968  0.10000751 0.10000923]\n",
      "166 1995\n",
      "softmax alpha-------------- [0.14285007 0.14286032 0.14286479 0.14285866 0.14285595 0.14285121\n",
      " 0.142859  ]\n",
      "167 957\n",
      "softmax alpha-------------- [0.05263656 0.05262625 0.05263741 0.0526273  0.05262908 0.05262557\n",
      " 0.05262855 0.05263324 0.05263152 0.05263068 0.05262675 0.05263208\n",
      " 0.05263491 0.05263407 0.05263499 0.05263371 0.05263638 0.05262826\n",
      " 0.05263269]\n",
      "168 1211\n",
      "softmax alpha-------------- [0.05882013 0.05882453 0.0588245  0.05882044 0.05882294 0.05882421\n",
      " 0.0588227  0.05882447 0.05882108 0.05882568 0.05882513 0.05882633\n",
      " 0.05882368 0.0588212  0.05882143 0.05882304 0.05882851]\n",
      "169 1281\n",
      "softmax alpha-------------- [0.05555638 0.05555494 0.05555451 0.05555666 0.0555547  0.05555647\n",
      " 0.0555562  0.05555545 0.05555472 0.05555555 0.05555439 0.0555579\n",
      " 0.05555543 0.0555544  0.05555723 0.05555649 0.05555437 0.05555422]\n",
      "170 81\n",
      "softmax alpha-------------- [0.06249787 0.06249935 0.06249989 0.06249685 0.06249932 0.06249847\n",
      " 0.06250164 0.06250398 0.06249858 0.06250295 0.06250242 0.06249881\n",
      " 0.06249913 0.06249505 0.06249902 0.06250667]\n",
      "171 1629\n",
      "softmax alpha-------------- [0.12500673 0.12499508 0.12499739 0.1250029  0.12500404 0.12499481\n",
      " 0.12500281 0.12499624]\n",
      "172 1713\n",
      "softmax alpha-------------- [0.04166703 0.04166714 0.04166731 0.0416643  0.04166627 0.04166627\n",
      " 0.04166724 0.04166723 0.04166552 0.04166719 0.04166752 0.04166632\n",
      " 0.0416674  0.04166942 0.04166796 0.04166542 0.04166579 0.04166347\n",
      " 0.04166645 0.0416681  0.04166477 0.04166772 0.04166701 0.04166716]\n",
      "173 161\n",
      "softmax alpha-------------- [0.14284758 0.1428579  0.14285289 0.14285739 0.14286366 0.14285633\n",
      " 0.14286426]\n",
      "174 1088\n",
      "softmax alpha-------------- [0.10000769 0.09999539 0.10000526 0.10000169 0.10000296 0.09999818\n",
      " 0.0999976  0.09999704 0.09999775 0.09999644]\n",
      "175 469\n",
      "softmax alpha-------------- [0.12498409 0.12500266 0.12500363 0.12500636 0.1249983  0.12500396\n",
      " 0.12499919 0.12500181]\n",
      "176 1417\n",
      "softmax alpha-------------- [0.0999985  0.09998678 0.10000288 0.10000289 0.10000478 0.10000113\n",
      " 0.09999613 0.10000583 0.10000228 0.09999879]\n",
      "177 490\n",
      "softmax alpha-------------- [0.0256401  0.02564141 0.02564057 0.02564221 0.02564107 0.02564138\n",
      " 0.02564136 0.0256404  0.025641   0.02564261 0.02563914 0.02564134\n",
      " 0.02564104 0.0256415  0.02564176 0.02564077 0.02563993 0.02564059\n",
      " 0.02564113 0.02564018 0.02564174 0.02564203 0.02564128 0.02563979\n",
      " 0.02564175 0.02564188 0.02564086 0.02563968 0.02564276 0.02564103\n",
      " 0.02564141 0.02563873 0.02564226 0.02564194 0.02563948 0.02564139\n",
      " 0.0256412  0.02564084 0.02564045]\n",
      "178 1142\n",
      "softmax alpha-------------- [0.05882045 0.05882378 0.05882518 0.0588234  0.05882311 0.05882042\n",
      " 0.05882347 0.05882739 0.0588236  0.05882694 0.05882304 0.05882308\n",
      " 0.05882486 0.05882322 0.05882244 0.05882185 0.05882379]\n",
      "179 1447\n",
      "softmax alpha-------------- [0.11111422 0.11111227 0.11111578 0.11110364 0.11110757 0.11111477\n",
      " 0.11111021 0.11110927 0.11111227]\n",
      "180 1929\n",
      "softmax alpha-------------- [0.14284702 0.14285743 0.14286112 0.14286333 0.14285424 0.14286854\n",
      " 0.14284832]\n",
      "181 567\n",
      "softmax alpha-------------- [0.04545333 0.04545342 0.04545174 0.04545323 0.04545717 0.0454548\n",
      " 0.04545153 0.04545658 0.04545361 0.04545377 0.04545746 0.04545462\n",
      " 0.04545704 0.0454542  0.04545147 0.04545429 0.04545432 0.04545686\n",
      " 0.04545361 0.04545529 0.04545597 0.04545572]\n",
      "182 1668\n",
      "softmax alpha-------------- [0.06250253 0.06249939 0.062502   0.06249703 0.06249972 0.06249806\n",
      " 0.0624987  0.06249987 0.06249936 0.06249963 0.06249931 0.06250016\n",
      " 0.06250052 0.0625025  0.06249995 0.06250126]\n",
      "183 1661\n",
      "softmax alpha-------------- [0.07691699 0.07692163 0.07692337 0.07692228 0.07692119 0.07692464\n",
      " 0.07692225 0.07692081 0.07692912 0.07692279 0.07692608 0.07692386\n",
      " 0.076925  ]\n",
      "184 1349\n",
      "softmax alpha-------------- [0.06666481 0.06667534 0.06667058 0.0666638  0.06666618 0.06666419\n",
      " 0.06666564 0.06667525 0.06667145 0.06666284 0.06666444 0.06666417\n",
      " 0.0666643  0.06666398 0.06666304]\n",
      "185 767\n",
      "softmax alpha-------------- [0.09090865 0.09091348 0.09091451 0.09090654 0.09090888 0.09090329\n",
      " 0.09091419 0.09091123 0.09090795 0.0909064  0.09090488]\n",
      "186 1936\n",
      "softmax alpha-------------- [0.07144076 0.07142078 0.07141617 0.07142066 0.07142412 0.07143949\n",
      " 0.07141539 0.0714192  0.07142455 0.07143786 0.07143737 0.0714358\n",
      " 0.07142896 0.07143889]\n",
      "187 1963\n",
      "softmax alpha-------------- [0.12500157 0.12500156 0.12500145 0.12500212 0.12499329 0.12500394\n",
      " 0.12500287 0.12499319]\n",
      "188 682\n",
      "softmax alpha-------------- [0.14286729 0.14285184 0.14286122 0.14285561 0.14285431 0.14286057\n",
      " 0.14284915]\n",
      "189 1341\n",
      "softmax alpha-------------- [0.07692161 0.07692167 0.07692334 0.07693069 0.07691871 0.07692365\n",
      " 0.07692364 0.07692503 0.07691983 0.0769247  0.07692084 0.07692191\n",
      " 0.07692438]\n",
      "190 825\n",
      "softmax alpha-------------- [0.14283853 0.142846   0.14287575 0.14286891 0.14287279 0.14284391\n",
      " 0.14285411]\n",
      "191 1792\n",
      "softmax alpha-------------- [0.05882579 0.05882511 0.05883138 0.05881838 0.05881798 0.05882603\n",
      " 0.0588241  0.05881784 0.05882748 0.05882274 0.05882316 0.05882563\n",
      " 0.05882522 0.05882641 0.05881894 0.05881653 0.05882727]\n",
      "192 967\n",
      "softmax alpha-------------- [0.11110518 0.11110176 0.11111375 0.11112065 0.11110721 0.11110823\n",
      " 0.11111022 0.11111285 0.11112015]\n",
      "193 239\n",
      "softmax alpha-------------- [0.1111082  0.11111867 0.11111158 0.11110836 0.11110931 0.11111137\n",
      " 0.11110931 0.11110869 0.1111145 ]\n",
      "194 1839\n",
      "softmax alpha-------------- [0.06250305 0.0625013  0.06249837 0.06249849 0.06249818 0.06250044\n",
      " 0.06249905 0.06249569 0.06250647 0.06249213 0.06249903 0.06250181\n",
      " 0.06250115 0.06250542 0.06249961 0.06249981]\n",
      "195 666\n",
      "softmax alpha-------------- [0.14284333 0.14285634 0.14286447 0.14285917 0.14286053 0.14285806\n",
      " 0.14285809]\n",
      "196 743\n",
      "softmax alpha-------------- [0.14286101 0.14285455 0.1428605  0.14285862 0.14285914 0.14285073\n",
      " 0.14285546]\n",
      "197 1310\n",
      "softmax alpha-------------- [0.06666768 0.06666881 0.06666395 0.06666787 0.06666678 0.06666636\n",
      " 0.06666203 0.06666465 0.06666681 0.06666843 0.06666754 0.0666656\n",
      " 0.0666689  0.06666736 0.06666725]\n",
      "198 461\n",
      "softmax alpha-------------- [0.14285426 0.14285042 0.14284979 0.14285643 0.14287475 0.14284859\n",
      " 0.14286577]\n",
      "199 1849\n",
      "softmax alpha-------------- [0.11111336 0.11111275 0.11111689 0.11112192 0.11111624 0.11110212\n",
      " 0.11111089 0.11110155 0.11110429]\n",
      "[-5.90645647e-01 -1.59282041e+00 -9.42021017e-01  5.99143536e-01\n",
      " -1.35193228e+00  1.18691785e-01 -1.31906388e+00 -2.34603620e+00\n",
      " -5.14540569e-01  1.37990351e+00 -1.15947146e+00 -1.21153388e+00\n",
      "  2.85816449e-01 -4.45174566e+00 -1.22542093e+00 -1.03175186e+00\n",
      "  4.79983037e-01 -9.97753061e-01 -1.14029431e+00  1.88885811e+00\n",
      " -8.49836082e-02 -1.75090107e+00 -9.55819428e-01 -4.46989798e+00\n",
      " -3.75730703e-01 -1.16014660e+00 -7.29276208e-04 -9.92606116e-01\n",
      " -1.20875672e+00 -1.99790926e+00 -7.48285501e-01  2.24010541e+00\n",
      " -2.89212095e+00 -6.76785623e-01  2.29803325e+00 -1.55436459e+00\n",
      " -1.13277502e+00 -1.42961441e+00 -2.58446961e+00 -9.75191924e-01\n",
      " -1.06523498e+00  4.43165862e+00 -7.98741347e-02  3.72376271e-01\n",
      " -4.05655975e-01 -1.94134177e+00 -2.95020050e-01 -2.10814358e+00\n",
      " -5.28647281e-01 -2.47829516e+00 -3.33563685e-02  1.00867434e-01\n",
      " -2.92075833e+00  2.08163633e+00 -5.13764772e-01 -2.49289815e+00\n",
      "  3.01199345e-01 -2.19569410e+00 -1.19238839e-01 -2.10702560e+00\n",
      " -1.71580331e+00  1.98363276e+00 -1.89874461e+00  4.62230202e-01\n",
      " -1.51772301e+00 -3.45253872e+00 -1.90023479e+00 -2.04137272e+00\n",
      " -7.28948738e-01  7.84841919e-01 -1.33696122e+00 -1.77053837e+00\n",
      " -2.10824331e+00 -1.57720024e+00 -2.07690399e+00  1.48063686e+00\n",
      " -1.30233773e-01 -6.68142964e-01 -2.63013944e-01 -2.87854182e+00\n",
      " -2.59872193e-01 -1.38247894e+00  5.33756639e-01 -2.85435444e+00\n",
      " -8.63571438e-01  4.32552728e+00 -1.43849549e+00 -3.16066759e-01\n",
      " -3.28996217e+00 -2.26344508e+00 -6.60824161e-01 -6.07415894e-01\n",
      " -2.84285898e+00 -7.94937283e-01 -1.46286134e-01 -2.60490068e+00\n",
      " -8.54779726e-01 -5.73034919e-01 -1.97592125e+00 -1.36236240e+00\n",
      " -3.74355425e+00 -8.67117187e-01 -1.43563945e+00  2.34576863e+00\n",
      " -2.96987698e+00  7.65086871e-02 -2.17461805e-02  5.15341283e-01\n",
      " -2.02670096e+00 -2.78001575e-01 -1.42098057e+00 -2.91791451e+00\n",
      "  2.17961743e-01 -4.24806211e+00 -4.22934690e+00 -2.08980842e-01\n",
      "  7.95310812e-01 -1.31311983e+00 -1.84694780e+00  2.80436376e-02\n",
      " -2.20531892e+00 -1.55285493e+00 -3.07808349e-01 -3.89489900e+00\n",
      "  9.35729989e-01 -2.88390969e+00  2.58226865e+00  3.67790058e+00\n",
      "  2.00635529e+00  2.49253275e+00  1.59183269e+00 -5.63067683e-01\n",
      "  1.25331578e+00 -6.98586581e-01 -1.47928831e+00  1.34061840e+00\n",
      "  1.22658963e+00 -9.78885152e-01 -1.51222162e+00 -1.03738116e+00\n",
      " -1.44147093e+00 -1.69721404e+00 -3.09655419e+00 -1.58183441e+00\n",
      " -7.67446343e-01  2.56534149e+00 -2.04566895e+00 -2.34351591e+00\n",
      " -4.37868224e+00 -2.22360175e+00 -9.18190080e-01 -5.96159654e-01\n",
      "  7.54521398e-01 -2.26445092e-01  7.88605558e-01 -7.94192369e-01\n",
      " -9.32280833e-01 -5.18641572e+00 -2.37243501e+00 -9.36014300e-01\n",
      "  8.06472622e-01 -1.55635746e+00  1.82163009e+00 -1.64540803e+00\n",
      "  3.09521064e-01  3.04954343e-01]\n"
     ]
    }
   ],
   "source": [
    "result=np.zeros((200, 166))\n",
    "RS=np.zeros((200, 166))\n",
    "#test_idx --> Test 的 index\n",
    "\n",
    "test_yes_id=[]\n",
    "for s in range(200):\n",
    "    print(s,test_idx[s])\n",
    "\n",
    "    yes=[]\n",
    "    sample=random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]])) #從training part 的positive feedback 取出YouTuber 當成Auxilary\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha=np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r =np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "    \n",
    "        alpha[a]=np.dot(A1[test_idx[s]],(relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T)+np.dot(Ay[test_idx[s]],np.expand_dims(Y[sample[a]],0).T)+np.dot(Aa[test_idx[s]],\n",
    "                            np.expand_dims(A[sample[a]],0).T)+ np.dot(Av[test_idx[s]],np.expand_dims(IGimg_npy[sample[a]],0).T))))*r\n",
    "    mul=np.zeros((1,32))\n",
    "    #print('alpha------------',alpha)\n",
    "    print('softmax alpha--------------',softmax(alpha))\n",
    "    for i in range(len(sample)):\n",
    "        mul+=softmax(alpha)[i]*A[sample[i]] #attention alpha*Ai part \n",
    "    new_mul=mul+U[test_idx[s]]  #(U+auxilary)\n",
    "    for k in range(166):\n",
    "        result[s][k]=np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T)+np.dot(B[test_idx[s]], IGimg_npy[k].T)\n",
    "print(RS[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取出test的資料\n",
    "testRS = np.zeros((200,10)) #shape 200*10\n",
    "\n",
    "#test_t 是true的\n",
    "#test_f 是false的\n",
    "        \n",
    "for z in range(200):\n",
    "    user_id = test_idx[z]\n",
    "    #positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    #not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "    #前兩個放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topN(sortlist,n):\n",
    "    topList = []\n",
    "    for i in range(n):\n",
    "        topList.append(sortlist.index(max(sortlist)))\n",
    "        #print(max(sortlist))\n",
    "        #print(sortlist.index(max(sortlist)))\n",
    "        sortlist[sortlist.index(max(sortlist))] = -1000000000\n",
    "    return topList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_accuarcy for count_0: 0.2775\n"
     ]
    }
   ],
   "source": [
    "count_0_all = []\n",
    "for i in range(len(testRS)):\n",
    "    top_0 = topN(list(testRS[i]),2)\n",
    "    count_0_all.append(top_0)\n",
    "    #print(top_0)\n",
    "\n",
    "acc_0 = 0\n",
    "total = len(count_0_all)*len(count_0_all[0])\n",
    "#print(total) #(200*2)\n",
    "for i in range(len(count_0_all)):\n",
    "    for j in range(len(count_0_all[i])):\n",
    "        if count_0_all[i][j] < 2: #代表是0或1 (也就是target)\n",
    "            acc_0 += 1\n",
    "#print(acc_0)\n",
    "avg_acc = acc_0/total\n",
    "print('avg_accuarcy for count_0:',avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing for dynamic length for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = tf.placeholder(tf.int32)\n",
    "i = tf.constant(0)\n",
    "\n",
    "while_condition = lambda i: tf.less(i, length)\n",
    "def body(i):\n",
    "    # do something here which you want to do in your loop\n",
    "    # increment i\n",
    "    return [tf.add(i, 1)]\n",
    "\n",
    "# do the loop:\n",
    "output = tf.while_loop(while_condition, body, [i])\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sess.run(output,feed_dict={length:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
