{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def newPath(path):\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "def writeProgress(msg, count, total):\n",
    "    sys.stdout.write(msg + \"{:.2%}\\r\".format(count/total))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)  \n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = exp_x / np.sum(exp_x)\n",
    "    return softmax_x \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All features: (165, 4876)\n",
      "Movie genre: (165, 20)\n",
      "User following: (1582, 165)\n",
      "User genre: (1582, 20)\n"
     ]
    }
   ],
   "source": [
    "all_npy = np.load('./npy/all_4876.npy')\n",
    "movie_genre = np.load('./npy/movie_genre.npy')\n",
    "usr_following = np.load('./npy/user_followings.npy')\n",
    "usr_genre = np.load('./npy/user_genre.npy')\n",
    "\n",
    "print('All features:', all_npy.shape)\n",
    "print('Movie genre:', movie_genre.shape)\n",
    "print('User following:', usr_following.shape)\n",
    "print('User genre:', usr_genre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582 165\n",
      "150 32\n"
     ]
    }
   ],
   "source": [
    "usr_nb = len(usr_following) # the number of users\n",
    "movie_nb = len(movie_genre)  # the number of movies\n",
    "\n",
    "print(usr_nb, movie_nb)\n",
    "\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "\n",
    "print(usr_test_amount, movie_test_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize usr_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1582, 20)\n"
     ]
    }
   ],
   "source": [
    "usr_genre_norm = np.zeros(usr_genre.shape)\n",
    "for i in range(len(usr_genre)):\n",
    "    usr_genre_norm[i] = usr_genre[i]/np.max(usr_genre[i])\n",
    "print(usr_genre_norm.shape)\n",
    "# print('Before:', usr_genre)\n",
    "# print('After:', usr_genre_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & testing split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followers: 1\n",
      "Max number of followers: 520\n",
      "Avg of followers: 142.0969696969697\n",
      "The num of followers over 5: 163\n"
     ]
    }
   ],
   "source": [
    "#The number of followers for each movie\n",
    "moive_followers = np.sum(usr_following, axis=0)\n",
    "# print(moive_followers)\n",
    "\n",
    "print('Min number of followers:', np.min(moive_followers))\n",
    "print('Max number of followers:', np.max(moive_followers))\n",
    "print('Avg of followers:', np.mean(moive_followers))\n",
    "\n",
    "asc = np.sort(moive_followers)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)\n",
    "\n",
    "over5 = 0\n",
    "for num in moive_followers:\n",
    "    if num >= 5:\n",
    "        over5 += 1\n",
    "print('The num of followers over 5:', over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 50: 125\n",
      "Over 100: 89\n",
      "Over 150: 58\n",
      "Over 200: 42\n",
      "Over 250: 31\n",
      "Over 300: 21\n"
     ]
    }
   ],
   "source": [
    "print('Over 50:', np.sum(moive_followers >= 50))\n",
    "print('Over 100:', np.sum(moive_followers >= 100))\n",
    "print('Over 150:', np.sum(moive_followers >= 150))\n",
    "print('Over 200:', np.sum(moive_followers >= 200))\n",
    "print('Over 250:', np.sum(moive_followers >= 250))\n",
    "print('Over 300:', np.sum(moive_followers >= 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42,) [  0   2   3   4   9  12  24  28  30  34  40  44  49  55  57  58  60  66\n",
      "  68  78  80  81  84  86  87  99 101 102 112 119 122 123 125 126 127 128\n",
      " 129 134 144 156 161 164]\n",
      "32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]\n"
     ]
    }
   ],
   "source": [
    "over200_idx = np.nonzero(moive_followers >= 200)[0]\n",
    "print(over200_idx.shape, over200_idx)\n",
    "\n",
    "random.seed(42)\n",
    "movie_test_idx = sorted(random.sample(list(over200_idx), movie_test_amount))\n",
    "print(len(movie_test_idx), movie_test_idx) # 32 [0, 2, 3, 12, 24, 28, 30, 44, 49, 55, 57, 58, 60, 66, 78, 80, 81, 84, 86, 87, 102, 112, 119, 122, 123, 125, 127, 128, 129, 144, 161, 164]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min number of followings: 10\n",
      "Max number of followings: 133\n",
      "Avg of followers: 14.820480404551201\n"
     ]
    }
   ],
   "source": [
    "#The number of following movie for each user\n",
    "each_user = np.sum(usr_following, axis=1)\n",
    "# print(each_user)\n",
    "\n",
    "print('Min number of followings:', np.min(each_user))\n",
    "print('Max number of followings:', np.max(each_user))\n",
    "print('Avg of followers:', np.mean(each_user))\n",
    "\n",
    "asc = np.sort(each_user)\n",
    "# print(each_user)\n",
    "# print(asc)\n",
    "desc = np.flip(asc)\n",
    "# print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over 10: 1582\n",
      "Over 12: 937\n",
      "Over 14: 613\n",
      "Over 16: 440\n",
      "Over 18: 315\n",
      "Over 20: 229\n"
     ]
    }
   ],
   "source": [
    "print('Over 10:', np.sum(each_user >= 10))\n",
    "print('Over 12:', np.sum(each_user >= 12))\n",
    "print('Over 14:', np.sum(each_user >= 14))\n",
    "print('Over 16:', np.sum(each_user >= 16))\n",
    "print('Over 18:', np.sum(each_user >= 18))\n",
    "print('Over 20:', np.sum(each_user >= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1582\n",
      "150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]\n"
     ]
    }
   ],
   "source": [
    "usr_idx = [i for i in range(len(usr_following))]\n",
    "print(len(usr_idx))\n",
    "\n",
    "random.seed(42)\n",
    "test_idx = sorted(random.sample(usr_idx, usr_test_amount))\n",
    "print(len(test_idx), test_idx[:10]) # 150 [13, 51, 54, 61, 65, 88, 93, 96, 114, 130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# init\n",
    "train_t = []\n",
    "train_f = []\n",
    "test_t = []\n",
    "test_f = []\n",
    "\n",
    "for i in range(usr_nb):\n",
    "    # init\n",
    "    t_for_train = []\n",
    "    f_for_train = []\n",
    "    t_for_test = []\n",
    "    f_for_test = []\n",
    "    \n",
    "    if i not in test_idx: #if not in test id, just append it to true or false list\n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                t_for_train.append(j)\n",
    "            else:\n",
    "                f_for_train.append(j)\n",
    "                \n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "#         print(len(t_for_train) + len(f_for_train))\n",
    "        \n",
    "    else: #if in test id, choose half of true and other \n",
    "        temp_t = []\n",
    "        temp_f = []\n",
    "        \n",
    "        for j in range(movie_nb):\n",
    "            if usr_following[i][j] == 1:\n",
    "                temp_t.append(j)\n",
    "            else:\n",
    "                temp_f.append(j)\n",
    "        \n",
    "        # random choose half true and half false for test \n",
    "        t_for_test = random.sample(temp_t, math.ceil(0.5*len(temp_t)))\n",
    "        f_for_test  = random.sample(temp_f, movie_test_amount-len(t_for_test))\n",
    "        \n",
    "        test_t.append(t_for_test)\n",
    "        test_f.append(f_for_test)\n",
    "        \n",
    "        #the others for training\n",
    "        t_for_train = [item for item in temp_t if not item in t_for_test]\n",
    "        f_for_train = [item for item in temp_f if not item in f_for_test]\n",
    "        train_t.append(t_for_train)\n",
    "        train_f.append(f_for_train)\n",
    "        \n",
    "    if not (len(t_for_train) + len(f_for_train) + len(t_for_test) + len(f_for_test)) == movie_nb:\n",
    "        print('Error!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train_t: 1582\n",
      "The length of train_f: 1582\n",
      "The length of test_t: 150\n",
      "The length of test_f: 150\n"
     ]
    }
   ],
   "source": [
    "print('The length of train_t:',len(train_t))\n",
    "print('The length of train_f:',len(train_f))\n",
    "print('The length of test_t:',len(test_t))\n",
    "print('The length of test_f:',len(test_f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 14.139064475347661\n",
      "Testing: 7.1866666666666665\n"
     ]
    }
   ],
   "source": [
    "#average num of following for training user\n",
    "total_train = 0\n",
    "for t in train_t:\n",
    "    total_train += len(t)\n",
    "avg = total_train / usr_nb\n",
    "print('Training:', avg)\n",
    "\n",
    "#average num of following for testing user\n",
    "total_test = 0\n",
    "for t in test_t:\n",
    "    total_test += len(t)\n",
    "avg = total_test / usr_test_amount\n",
    "print('Testing:', avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_auxilary = [i for i in range(movie_nb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAVE_NAME = 'MRM_ALL_Embedding200_L2_resplit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 4876 200\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128 # latent dims\n",
    "ft_dim = all_npy.shape[1] # feature dims\n",
    "embedding_dims = 200\n",
    "\n",
    "print(latent_dim, ft_dim, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "user = tf.placeholder(tf.int32,shape=(1,))\n",
    "i = tf.placeholder(tf.int32, shape=(1,))\n",
    "j = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "#多少個auxliary \n",
    "xf = tf.placeholder(tf.float32, shape=(None, ft_dim))\n",
    "l_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "l_id_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "r = tf.placeholder(tf.float32,shape=(None,))\n",
    "positive_id = tf.placeholder(tf.int32, shape=(None,))\n",
    "positive_len = tf.placeholder(tf.int32,shape=(1,))\n",
    "\n",
    "image_i = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "image_j = tf.placeholder(tf.float32, [1, ft_dim])\n",
    "\n",
    "with tf.variable_scope(\"item_level\"):\n",
    "    user_latent = tf.get_variable(\"user_latent\", [usr_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    item_latent = tf.get_variable(\"item_latent\", [movie_nb, latent_dim],\n",
    "                                  initializer=tf.random_normal_initializer(0,0.1,seed=3)) \n",
    "    aux_item = tf.get_variable(\"aux_item\", [movie_nb, latent_dim],\n",
    "                               initializer=tf.random_normal_initializer(0,0.1,seed=3))\n",
    "    \n",
    "#     W1 = tf.get_variable(\"W1\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wu = tf.get_variable(\"Wu\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wy = tf.get_variable(\"Wy\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wa = tf.get_variable(\"Wa\", [usr_nb, movie_nb, latent_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wv = tf.get_variable(\"Wv\", [usr_nb, movie_nb, embedding_dims], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#     Wve = tf.get_variable(\"Wve\", [embedding_dims, ft_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    aux_new = tf.get_variable(\"aux_new\", [1, latent_dim], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "with tf.variable_scope('feature_level'):\n",
    "    embedding = tf.get_variable(\"embedding\", [embedding_dims,ft_dim],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Beta = tf.get_variable(\"beta\", [usr_nb, embedding_dims],\n",
    "                           initializer=tf.random_normal_initializer(0.01, 0.001, seed=10))\n",
    "    \n",
    "#lookup the latent factors by user and id\n",
    "u = tf.nn.embedding_lookup(user_latent, user)\n",
    "vi = tf.nn.embedding_lookup(item_latent, i)\n",
    "vj = tf.nn.embedding_lookup(item_latent, j)\n",
    "\n",
    "# w1 = tf.nn.embedding_lookup(W1, user)\n",
    "wu = tf.squeeze(tf.nn.embedding_lookup(Wu, user))\n",
    "wy = tf.squeeze(tf.nn.embedding_lookup(Wy, user))\n",
    "wa = tf.squeeze(tf.nn.embedding_lookup(Wa, user))\n",
    "wv = tf.squeeze(tf.nn.embedding_lookup(Wv, user))\n",
    "\n",
    "beta = tf.nn.embedding_lookup(Beta, user) #user feature latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_list = tf.Variable([])\n",
    "q = tf.constant(0)\n",
    "\n",
    "def att_cond(q,a_list):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def att_body(q,a_list):\n",
    "    xfi = tf.expand_dims(xf[q],0) #(1,l)\n",
    "    wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[q]),0) #取該YOUTUBER那欄(1,K)\n",
    "    \n",
    "    a_list = tf.concat([a_list,[(tf.nn.relu(tf.matmul(wuui, u, transpose_b=True) +\n",
    "                                            tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0), transpose_b=True) +\n",
    "                                            tf.matmul(wvui, tf.matmul(embedding,xfi, transpose_b=True)))[0][0])*r[q]]],0)\n",
    "    q += 1\n",
    "    return q, a_list\n",
    "\n",
    "_, a_list = tf.while_loop(att_cond,att_body,[q,a_list],shape_invariants=[q.get_shape(),tf.TensorShape([None])])\n",
    "\n",
    "a_list_smooth = tf.add(a_list,0.0000000001)\n",
    "a_list_soft = tf.divide(a_list_smooth,tf.reduce_sum(a_list_smooth, 0)) #without softmax\n",
    "\n",
    "norm_par = [wu,wy,wa,wv]\n",
    "\n",
    "wuui = tf.expand_dims(tf.nn.embedding_lookup(wu,l_id[-1]),0)\n",
    "wyui = tf.expand_dims(tf.nn.embedding_lookup(wy,l_id[-1]),0)\n",
    "waui = tf.expand_dims(tf.nn.embedding_lookup(wa,l_id[-1]),0)\n",
    "wvui = tf.expand_dims(tf.nn.embedding_lookup(wv,l_id[-1]),0)\n",
    "wu_be_relu = tf.matmul(wuui, u, transpose_b=True)\n",
    "wy_be_relu = tf.matmul(wyui, tf.expand_dims(tf.nn.embedding_lookup(item_latent,l_id[-1]),0), transpose_b=True)\n",
    "wa_be_relu = tf.matmul(waui, tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[-1]),0), transpose_b=True)\n",
    "wv_be_relu = tf.matmul(wvui, tf.matmul(embedding,tf.expand_dims(xf[-1],0), transpose_b=True))\n",
    "\n",
    "last_be_relu = [wu_be_relu,wy_be_relu,wa_be_relu,wv_be_relu]\n",
    "\n",
    "aux_np = tf.expand_dims(tf.zeros(latent_dim),0)\n",
    "q = tf.constant(0)\n",
    "\n",
    "def sum_att_cond(q,aux_np):\n",
    "    return tf.less(q,l_id_len[0])\n",
    "\n",
    "def sum_att_body(q,aux_np):\n",
    "    aux_np = tf.math.add_n([aux_np,a_list_soft[q]*tf.expand_dims(tf.nn.embedding_lookup(aux_item, l_id[q]),0)]) \n",
    "    q += 1\n",
    "    return q, aux_np\n",
    "\n",
    "_, aux_np = tf.while_loop(sum_att_cond, sum_att_body, [q,aux_np])\n",
    "\n",
    "aux_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "aux_np += u #user_latent factor + sum (alpha*auxilary)\n",
    "aux_new = tf.assign(aux_new,aux_np) #把aux_new 的 值變成aux_np\n",
    "\n",
    "latent_i_part = tf.matmul(aux_new, vi, transpose_b=True)\n",
    "feature_i_part = tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "latent_j_part = tf.matmul(aux_new, vj, transpose_b=True)\n",
    "feature_j_part = tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "only_aux_i_part = tf.matmul(aux_np, vi, transpose_b=True)\n",
    "only_aux_j_part = tf.matmul(aux_np, vj, transpose_b=True)\n",
    "\n",
    "#矩陣中對應函數各自相乘\n",
    "# ex: tf.matmul(thetav,(tf.matmul(embedding, image_i, transpose_b=True)))\n",
    "xui = tf.matmul(aux_new, vi, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_i, transpose_b=True)))\n",
    "xuj = tf.matmul(aux_new, vj, transpose_b=True)+ tf.matmul(beta,(tf.matmul(embedding,image_j, transpose_b=True)))\n",
    "\n",
    "xuij = tf.subtract(xui,xuj)\n",
    "\n",
    "l2_norm = tf.add_n([\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(u, u)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vi, vi)),\n",
    "            0.0001 * tf.reduce_sum(tf.multiply(vj, vj)),\n",
    "  \n",
    "            0.01 * tf.reduce_sum(tf.multiply(wu, wu)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wy, wy)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wa, wa)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(wv,wv)),\n",
    "            \n",
    "            0.001 * tf.reduce_sum(tf.multiply(beta,beta)),\n",
    "            0.01 * tf.reduce_sum(tf.multiply(embedding,embedding))\n",
    "          ])\n",
    "\n",
    "loss = l2_norm - tf.log(tf.sigmoid(xuij)) # objective funtion\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss) #parameter optimize \n",
    "auc = tf.reduce_mean(tf.to_float(xuij > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start time:', time.ctime())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "loss_acc_list = []\n",
    "t0 = time.time()\n",
    "\n",
    "train_yes_id=[]\n",
    "\n",
    "for q in range(5):\n",
    "    print('Epoch:',q)\n",
    "    train_auc = 0\n",
    "    total_loss = 0\n",
    "    xuij_auc = 0\n",
    "    length = 0\n",
    "    \n",
    "    for z in range(usr_nb):\n",
    "        writeProgress('Progress:', z, usr_nb)\n",
    "        \"\"\"\n",
    "        yes 用來存放選擇到的YouTuber feature (for auxilary)\n",
    "        yesr 用來存放user對該YouTuber的喜好程度(user_category 跟 YouTuber_category的相似性)\n",
    "        r_3 用來存放user 對該YouTuber種類的偏好(取max)\n",
    "        \"\"\"\n",
    "        yes = []\n",
    "        yesr = []\n",
    "        \n",
    "#         #選全部的Positive\n",
    "#         sample = random.sample(train_t[z],len(train_t[z]))\n",
    "        #選全部的電影\n",
    "        sample = all_auxilary\n",
    "        \n",
    "        #change\n",
    "        r_3 = np.zeros(len(sample))\n",
    "         \n",
    "        for b in range(len(sample)):\n",
    "            yes.append(all_npy[sample[b]])\n",
    "            yesr.append(movie_genre[sample[b]] * usr_genre_norm[z])\n",
    "        \n",
    "        for b in range(len(yesr)):\n",
    "            r_3[b]=max(yesr[b])\n",
    "        #print('r_3:',r_3)\n",
    "        \n",
    "        yes = np.array(yes)\n",
    "        \n",
    "        # positive sample\n",
    "        train_t_sample = train_t[z]\n",
    "        for ta in train_t_sample:\n",
    "            #print(ta,'--> positive feedback')\n",
    "            \n",
    "            pos = sample.index(ta)\n",
    "            \n",
    "            image_1=np.expand_dims(all_npy[ta],0)\n",
    "            train_f_sample = random.sample(train_f[z],10)\n",
    "            \n",
    "            for b in train_f_sample:\n",
    "                image_2 = np.expand_dims(all_npy[b],0)\n",
    "                \n",
    "                _last_be_relu, _norm_par, _a_list, r3, _auc, _loss, _ = sess.run(\n",
    "                    [last_be_relu, norm_par, a_list_smooth, a_list_soft, auc, loss, train_op], \n",
    "                    feed_dict={user: [z], i: [ta], j: [b], xf: yes, \n",
    "                               l_id:sample, l_id_len:[len(sample)],\n",
    "                               positive_id: train_t[z], positive_len:[len(train_t[z])],\n",
    "                               r: r_3, image_i: image_1, image_j: image_2})\n",
    "\n",
    "                train_auc += _auc\n",
    "                total_loss += _loss\n",
    "                length += 1\n",
    "    \n",
    "    print(\"{:<20}{}\".format('total_loss', total_loss/length))\n",
    "    print(\"{:<20}{}\".format('train_auc:', train_auc/length))\n",
    "    \n",
    "    loss_acc_list.append([total_loss/length, train_auc/length])\n",
    "    \n",
    "    print('\\tCurrent time:', time.ctime(), ' sec')\n",
    "    print('==================================================')\n",
    "    \n",
    "print('Total cost time:',time.time()-t0, ' sec')\n",
    "\n",
    "print('End time:', time.ctime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(loss_acc_list)):\n",
    "    print('Iteration:',i)\n",
    "    print('loss=',loss_acc_list[i][0])\n",
    "    print('acc=',loss_acc_list[i][1])\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training history\n",
    "epochs = range(1, len(loss_acc_list) + 1)\n",
    "print(epochs)\n",
    "loss = [ls[0].tolist()[0][0] for ls in loss_acc_list]\n",
    "print(loss)\n",
    "acc = [ls[1] for ls in loss_acc_list]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製結果\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.title('Training accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get latent factor and Each weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U, Y, A, E, Au, Ay, Aa, Av, B = sess.run([user_latent, item_latent, aux_item, embedding, Wu, Wy, Wa, Wv, Beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Wu weight shape:', Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.savez('./weight/' + SAVE_NAME + '.npz', \n",
    "         U=U, Y=Y, A=A, E=E, Wu=Au, Wy=Ay, Wa=Aa, Wv=Av, B=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numpy.lib.npyio.NpzFile object at 0x7f36fcb0dbe0>\n",
      "User latent shape:  (1582, 128)\n",
      "photo latent shape:  (165, 128)\n",
      "Auxilary latent shape:  (165, 128)\n",
      "Embedding shape: (200, 4876)\n",
      "Wu weight shape: (1582, 165, 128)\n",
      "Wy weight shape: (1582, 165, 128)\n",
      "Wa weight shape: (1582, 165, 128)\n",
      "Wv weight shape: (1582, 165, 200)\n",
      "Beta shape: (1582, 200)\n"
     ]
    }
   ],
   "source": [
    "# reload params if crash\n",
    "SAVE_NAME = 'MRM_ALL_Embedding200_L2_resplit'\n",
    "\n",
    "params = np.load('./weight/' + SAVE_NAME + '.npz')\n",
    "print(params)\n",
    "U = params['U']\n",
    "Y = params['Y']\n",
    "A = params['A']\n",
    "E = params['E']\n",
    "Au = params['Wu']\n",
    "Ay = params['Wy']\n",
    "Aa = params['Wa']\n",
    "Av = params['Wv']\n",
    "B = params['B']\n",
    "\n",
    "print('User latent shape: ',U.shape)\n",
    "print('photo latent shape: ', Y.shape)\n",
    "print('Auxilary latent shape: ',A.shape)\n",
    "print('Embedding shape:', E.shape)\n",
    "print('Wu weight shape:', Au.shape)\n",
    "print('Wy weight shape:', Ay.shape)\n",
    "print('Wa weight shape:', Aa.shape)\n",
    "print('Wv weight shape:', Av.shape)\n",
    "print('Beta shape:',B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "alpha:         [0.04003703 0.0083279  0.07587001 0.03836857 0.01493908 0.01253587\n",
      " 0.01782649]\n",
      "softmax alpha: [0.14430585 0.13980182 0.14957051 0.14406528 0.14072914 0.14039134\n",
      " 0.14113607]\n",
      "==================================================\n",
      "1 51\n",
      "alpha:         [0.01496949 0.1221801  0.00983452 0.0777931  0.05591541 0.11530561]\n",
      "softmax alpha: [0.15822224 0.17612804 0.15741185 0.16848121 0.16483526 0.1749214 ]\n",
      "==================================================\n",
      "2 54\n",
      "alpha:         [0.05315409 0.05639275 0.05493165 0.1418067  0.07021926 0.04608311]\n",
      "softmax alpha: [0.16372267 0.16425377 0.16401396 0.17889992 0.1665406  0.16256907]\n",
      "==================================================\n",
      "3 61\n",
      "alpha:         [0.09592985 0.07227526 0.07595961 0.04747918 0.06705877]\n",
      "softmax alpha: [0.20487198 0.20008269 0.20082122 0.19518243 0.19904168]\n",
      "==================================================\n",
      "4 65\n",
      "alpha:         [0.03305649 0.13434295 0.04390337 0.0876186  0.1236389  0.08623225]\n",
      "softmax alpha: [0.15815267 0.17501073 0.15987747 0.16702156 0.1731474  0.16679017]\n",
      "==================================================\n",
      "5 88\n",
      "alpha:         [0.00897702 0.00875871 0.00910912 0.00767562 0.00659969 0.00673652\n",
      " 0.00824521 0.00639231 0.00922306]\n",
      "softmax alpha: [0.11122315 0.11119887 0.11123784 0.1110785  0.11095905 0.11097423\n",
      " 0.11114179 0.11093604 0.11125052]\n",
      "==================================================\n",
      "6 93\n",
      "alpha:         [0.03492491 0.04028509 0.02809643 0.02676692 0.01008987 0.01868514\n",
      " 0.02193556]\n",
      "softmax alpha: [0.14415658 0.14493136 0.14317557 0.14298534 0.14062054 0.14183442\n",
      " 0.14229619]\n",
      "==================================================\n",
      "7 96\n",
      "alpha:         [0.06457178 0.09929317 0.047402   0.09699532 0.06366734]\n",
      "softmax alpha: [0.19800573 0.20500151 0.19463504 0.20453099 0.19782673]\n",
      "==================================================\n",
      "8 114\n",
      "alpha:         [0.04999655 0.0752644  0.08983603 0.06471837 0.07476241]\n",
      "softmax alpha: [0.19584268 0.20085425 0.20380245 0.19874717 0.20075345]\n",
      "==================================================\n",
      "9 130\n",
      "alpha:         [0.05043874 0.08969856 0.13985817 0.10963186 0.13315019]\n",
      "softmax alpha: [0.18936567 0.19694799 0.20707878 0.20091321 0.20569435]\n",
      "==================================================\n",
      "10 135\n",
      "alpha:         [0.05725089 0.07680239 0.04747117 0.06226855 0.09453027]\n",
      "softmax alpha: [0.19790125 0.20180859 0.19597527 0.19889675 0.20541813]\n",
      "==================================================\n",
      "11 142\n",
      "alpha:         [0.00944889 0.01459691 0.01958296 0.06473661 0.02874345 0.05364545]\n",
      "softmax alpha: [0.16294975 0.16379078 0.16460949 0.17221257 0.16612432 0.17031309]\n",
      "==================================================\n",
      "12 146\n",
      "alpha:         [0.10386158 0.10180496 0.05410585 0.09080715 0.12722489]\n",
      "softmax alpha: [0.20160981 0.2011956  0.19182404 0.19899501 0.20637554]\n",
      "==================================================\n",
      "13 161\n",
      "alpha:         [0.06280893 0.06136093 0.10039983 0.09009187 0.09780404]\n",
      "softmax alpha: [0.19607333 0.19578963 0.20358419 0.20149643 0.20305642]\n",
      "==================================================\n",
      "14 163\n",
      "alpha:         [0.03883152 0.05913285 0.03645782 0.10634949 0.02154864 0.05327783]\n",
      "softmax alpha: [0.16432762 0.16769779 0.16393802 0.17580582 0.16151197 0.16671878]\n",
      "==================================================\n",
      "15 178\n",
      "alpha:         [0.17670757 0.07771559 0.18908745 0.11924163 0.10294332]\n",
      "softmax alpha: [0.20871374 0.18904246 0.21131365 0.19705792 0.19387224]\n",
      "==================================================\n",
      "16 186\n",
      "alpha:         [0.04265338 0.02347631 0.04585412 0.05105302 0.04964164 0.06039483]\n",
      "softmax alpha: [0.16618034 0.16302385 0.16671309 0.16758207 0.16734572 0.16915493]\n",
      "==================================================\n",
      "17 189\n",
      "alpha:         [0.0613076  0.09749196 0.07431836 0.12573927 0.15596242]\n",
      "softmax alpha: [0.19172607 0.1987906  0.19423687 0.20448596 0.21076051]\n",
      "==================================================\n",
      "18 191\n",
      "alpha:         [0.21358047 0.2125837  0.07150291 0.10050481 0.10654043]\n",
      "softmax alpha: [0.2146784  0.21446452 0.1862451  0.19172565 0.19288633]\n",
      "==================================================\n",
      "19 198\n",
      "alpha:         [0.10254091 0.20818399 0.17389529 0.16511297 0.23215663]\n",
      "softmax alpha: [0.18558561 0.20626451 0.19931184 0.19756909 0.21126896]\n",
      "==================================================\n",
      "20 206\n",
      "alpha:         [0.06310693 0.04569025 0.04710294 0.05165318 0.08976331]\n",
      "softmax alpha: [0.20070311 0.19723779 0.19751662 0.19841742 0.20612507]\n",
      "==================================================\n",
      "21 209\n",
      "alpha:         [0.07917746 0.16888645 0.05650984 0.1147482  0.02380039 0.03424724]\n",
      "softmax alpha: [0.16639359 0.18201062 0.16266427 0.17241886 0.15742969 0.15908296]\n",
      "==================================================\n",
      "22 224\n",
      "alpha:         [0.11507604 0.13997591 0.05550432 0.03455546 0.0768824  0.05941623]\n",
      "softmax alpha: [0.17246126 0.17680943 0.16248747 0.15911895 0.16599854 0.16312435]\n",
      "==================================================\n",
      "23 228\n",
      "alpha:         [0.06973366 0.04422075 0.05298253 0.05395924 0.07195566 0.02769024]\n",
      "softmax alpha: [0.16938817 0.16512124 0.16657436 0.16673713 0.16976496 0.16241414]\n",
      "==================================================\n",
      "24 255\n",
      "alpha:         [0.03715605 0.02250729 0.02319063 0.03709316 0.0211254  0.01759135\n",
      " 0.02848208 0.01292347]\n",
      "softmax alpha: [0.12652348 0.12468357 0.1247688  0.12651552 0.12451139 0.12407214\n",
      " 0.12543076 0.12349434]\n",
      "==================================================\n",
      "25 283\n",
      "alpha:         [0.041256   0.06738862 0.02816046 0.02456271 0.02995279 0.05867066\n",
      " 0.01680909]\n",
      "softmax alpha: [0.14328517 0.14707894 0.14142101 0.14091312 0.14167471 0.14580229\n",
      " 0.13982476]\n",
      "==================================================\n",
      "26 285\n",
      "alpha:         [0.01729634 0.00934074 0.00309151 0.0073515  0.01699904 0.00832146\n",
      " 0.02136437 0.00633543 0.00391292]\n",
      "softmax alpha: [0.11187278 0.11098629 0.11029488 0.11076573 0.11183952 0.11087323\n",
      " 0.11232881 0.11065325 0.11038551]\n",
      "==================================================\n",
      "27 292\n",
      "alpha:         [0.0753137  0.09737795 0.04711417 0.04097152 0.03098015 0.04169332]\n",
      "softmax alpha: [0.16994323 0.17373457 0.16521785 0.16420609 0.16257361 0.16432465]\n",
      "==================================================\n",
      "28 313\n",
      "alpha:         [0.04412503 0.09695563 0.1422841  0.0955075  0.06258616]\n",
      "softmax alpha: [0.1912504  0.20162593 0.21097563 0.20133416 0.19481389]\n",
      "==================================================\n",
      "29 318\n",
      "alpha:         [0.0033796  0.00087279 0.00294384 0.00181319 0.00688613 0.00525333\n",
      " 0.00190157 0.00088239 0.00176457 0.00301883 0.00227907 0.00104601]\n",
      "softmax alpha: [0.08339235 0.08318356 0.08335602 0.08326183 0.08368528 0.08354875\n",
      " 0.08326919 0.08318436 0.08325778 0.08336227 0.08330063 0.08319797]\n",
      "==================================================\n",
      "30 326\n",
      "alpha:         [0.01204898 0.06499852 0.00707343 0.04092433 0.03004975 0.02063536\n",
      " 0.04172154]\n",
      "softmax alpha: [0.14014211 0.14776254 0.13944656 0.14424776 0.14268762 0.14135061\n",
      " 0.1443628 ]\n",
      "==================================================\n",
      "31 327\n",
      "alpha:         [0.00609909 0.00548996 0.00874094 0.0036199  0.01081406 0.00404252\n",
      " 0.00564744 0.0045402  0.00675651 0.01082711]\n",
      "softmax alpha: [0.09994384 0.09988298 0.10020822 0.09969636 0.10041618 0.09973851\n",
      " 0.09989871 0.09978816 0.10000956 0.10041749]\n",
      "==================================================\n",
      "32 333\n",
      "alpha:         [0.16913414 0.0685216  0.20192817 0.05291168 0.06402986]\n",
      "softmax alpha: [0.21150016 0.19125607 0.21855109 0.18829376 0.19039892]\n",
      "==================================================\n",
      "33 334\n",
      "alpha:         [0.03758366 0.10004308 0.04687234 0.06988773 0.0295776  0.04524035]\n",
      "softmax alpha: [0.16376457 0.17431941 0.16529282 0.16914121 0.1624587  0.16502328]\n",
      "==================================================\n",
      "34 350\n",
      "alpha:         [0.04153153 0.0335627  0.03555842 0.09111589 0.05248473 0.02701452]\n",
      "softmax alpha: [0.1657401  0.16442459 0.16475306 0.17416537 0.16756546 0.16335143]\n",
      "==================================================\n",
      "35 393\n",
      "alpha:         [0.0015062  0.00092816 0.02688893 0.00149253 0.00037695 0.00073037\n",
      " 0.00052974 0.00111947 0.0005105  0.00103065 0.00091706 0.00085845\n",
      " 0.00026305 0.00092065 0.00967284]\n",
      "softmax alpha: [0.06655346 0.066515   0.06826439 0.06655255 0.06647835 0.06650185\n",
      " 0.06648851 0.06652773 0.06648723 0.06652182 0.06651426 0.06651037\n",
      " 0.06647078 0.0665145  0.06709921]\n",
      "==================================================\n",
      "36 407\n",
      "alpha:         [0.09847613 0.15915812 0.10592046 0.09540631 0.09474518]\n",
      "softmax alpha: [0.19750184 0.20985775 0.1989776  0.19689647 0.19676634]\n",
      "==================================================\n",
      "37 429\n",
      "alpha:         [0.06603503 0.06032093 0.03577919 0.10328379 0.01604295]\n",
      "softmax alpha: [0.20186981 0.2007196  0.19585355 0.20953101 0.19202603]\n",
      "==================================================\n",
      "38 432\n",
      "alpha:         [0.08545617 0.07776301 0.06429675 0.03717821 0.08712637 0.00895907]\n",
      "softmax alpha: [0.17087313 0.16956362 0.16729553 0.16281969 0.17115876 0.15828928]\n",
      "==================================================\n",
      "39 435\n",
      "alpha:         [0.05346676 0.07540901 0.10145826 0.09307356 0.06520057]\n",
      "softmax alpha: [0.19517713 0.19950709 0.20477238 0.2030626  0.19748079]\n",
      "==================================================\n",
      "40 440\n",
      "alpha:         [0.00206818 0.00165329 0.00253746 0.00095635 0.00167899 0.00770746\n",
      " 0.00295146 0.00219156 0.00081676 0.00196863 0.00152773 0.00135706\n",
      " 0.00137472]\n",
      "softmax alpha: [0.07691171 0.0768798  0.07694781 0.07682624 0.07688178 0.07734666\n",
      " 0.07697967 0.0769212  0.07681552 0.07690405 0.07687015 0.07685703\n",
      " 0.07685839]\n",
      "==================================================\n",
      "41 447\n",
      "alpha:         [0.03981273 0.02270066 0.03920346 0.07911534 0.04605166 0.04224152]\n",
      "softmax alpha: [0.16580449 0.16299137 0.1657035  0.17245079 0.16684217 0.16620768]\n",
      "==================================================\n",
      "42 449\n",
      "alpha:         [3.53414747e-04 9.97980037e-05 1.12732014e-03 2.14700720e-04\n",
      " 1.25641408e-03 3.62092322e-04 3.66656791e-04 2.26688324e-04\n",
      " 1.95626614e-04 9.44442563e-04 3.57286185e-04 1.03370392e-03\n",
      " 2.31405235e-03 4.93221060e-04 1.20773203e-04 2.24051946e-04\n",
      " 6.33074199e-03]\n",
      "softmax alpha: [0.05878883 0.05877392 0.05883434 0.05878068 0.05884194 0.05878934\n",
      " 0.05878961 0.05878138 0.05877955 0.05882359 0.05878906 0.05882884\n",
      " 0.05890421 0.05879705 0.05877516 0.05878123 0.05914128]\n",
      "==================================================\n",
      "43 451\n",
      "alpha:         [5.28427554e-03 1.03526983e-03 1.99845756e-05 6.04949658e-05\n",
      " 7.16115360e-03 1.08778927e-04 3.19160244e-03 1.63648699e-04\n",
      " 3.03738275e-05 8.29745473e-04 9.34441208e-06 1.45751065e-03\n",
      " 3.23603827e-04 4.58018412e-03 1.43761035e-04 1.81003318e-03\n",
      " 8.74433686e-04 2.48061078e-03 3.25077949e-04 1.08016777e-03]\n",
      "softmax alpha: [0.05018704 0.04997425 0.04992354 0.04992556 0.05028132 0.04992797\n",
      " 0.05008212 0.04993071 0.04992405 0.04996398 0.049923   0.04999535\n",
      " 0.0499387  0.05015172 0.04992971 0.05001298 0.04996621 0.05004653\n",
      " 0.04993877 0.04997649]\n",
      "==================================================\n",
      "44 457\n",
      "alpha:         [0.09032638 0.12336692 0.19067447 0.08893833 0.09491876]\n",
      "softmax alpha: [0.19446309 0.20099558 0.21498978 0.19419335 0.19535819]\n",
      "==================================================\n",
      "45 466\n",
      "alpha:         [0.13668603 0.12650892 0.09651006 0.09432239 0.09566742 0.03565675]\n",
      "softmax alpha: [0.17322815 0.17147413 0.16640649 0.16604285 0.16626633 0.15658206]\n",
      "==================================================\n",
      "46 469\n",
      "alpha:         [0.01226998 0.01968614 0.02665125 0.02897857 0.03230917 0.00348956\n",
      " 0.02512293 0.02417185]\n",
      "softmax alpha: [0.12383616 0.12475796 0.12562994 0.12592267 0.12634276 0.12275358\n",
      " 0.12543809 0.12531884]\n",
      "==================================================\n",
      "47 476\n",
      "alpha:         [0.11777769 0.04716831 0.10927659 0.06083839 0.05076106]\n",
      "softmax alpha: [0.20819489 0.19400138 0.20643251 0.1966716  0.19469963]\n",
      "==================================================\n",
      "48 501\n",
      "alpha:         [0.12370362 0.08653109 0.0893968  0.0522074  0.05059454 0.03628171]\n",
      "softmax alpha: [0.17523647 0.16884208 0.16932662 0.16314512 0.1628822  0.1605675 ]\n",
      "==================================================\n",
      "49 505\n",
      "alpha:         [0.07632185 0.04842419 0.04008217 0.09100197 0.05508546 0.04464493]\n",
      "softmax alpha: [0.16950611 0.16484264 0.16347323 0.17201283 0.16594436 0.16422083]\n",
      "==================================================\n",
      "50 514\n",
      "alpha:         [0.21413037 0.10931552 0.10995911 0.10057695 0.06553965]\n",
      "softmax alpha: [0.21948401 0.19764343 0.19777067 0.19592383 0.18917806]\n",
      "==================================================\n",
      "51 538\n",
      "alpha:         [0.22180491 0.0898436  0.17897867 0.08965609 0.05616044]\n",
      "softmax alpha: [0.21939395 0.19227135 0.21019649 0.1922353  0.1859029 ]\n",
      "==================================================\n",
      "52 541\n",
      "alpha:         [0.00084438 0.00033064 0.02687462 0.00088676 0.00064865 0.00055378\n",
      " 0.00080319 0.00094658 0.00094815 0.00056028 0.00019161 0.00016471\n",
      " 0.00047272 0.00070113 0.00095068 0.00023739]\n",
      "softmax alpha: [0.06241049 0.06237843 0.06405638 0.06241313 0.06239827 0.06239235\n",
      " 0.06240792 0.06241687 0.06241696 0.06239276 0.06236976 0.06236808\n",
      " 0.0623873  0.06240155 0.06241712 0.06237262]\n",
      "==================================================\n",
      "53 542\n",
      "alpha:         [0.10686242 0.04347019 0.07037794 0.05239286 0.15043697]\n",
      "softmax alpha: [0.20432042 0.19177009 0.19700024 0.19348885 0.21342041]\n",
      "==================================================\n",
      "54 546\n",
      "alpha:         [0.01809458 0.02960453 0.06450588 0.04459915 0.05550366 0.04193787\n",
      " 0.0125104 ]\n",
      "softmax alpha: [0.14000466 0.14162542 0.1466556  0.14376504 0.1453413  0.14338295\n",
      " 0.13922503]\n",
      "==================================================\n",
      "55 548\n",
      "alpha:         [0.06460089 0.03322711 0.07229111 0.10947621 0.11287304 0.0353125 ]\n",
      "softmax alpha: [0.16547187 0.16036099 0.16674929 0.17306661 0.17365549 0.16069575]\n",
      "==================================================\n",
      "56 552\n",
      "alpha:         [0.15722455 0.10139632 0.08577639 0.06320796 0.17013177]\n",
      "softmax alpha: [0.20833313 0.19702097 0.19396743 0.18963891 0.21103956]\n",
      "==================================================\n",
      "57 563\n",
      "alpha:         [0.01292676 0.08191588 0.08116794 0.05627097 0.07398979 0.08115839]\n",
      "softmax alpha: [0.15822974 0.16953123 0.16940447 0.16523889 0.16819282 0.16940286]\n",
      "==================================================\n",
      "58 569\n",
      "alpha:         [0.06565958 0.06684794 0.05265909 0.03337306 0.02900772 0.06362293]\n",
      "softmax alpha: [0.16896233 0.16916324 0.16677995 0.16359425 0.16288166 0.16861856]\n",
      "==================================================\n",
      "59 592\n",
      "alpha:         [0.01484216 0.02817904 0.01712434 0.01028275 0.01784602 0.01544753\n",
      " 0.01734696 0.01820704]\n",
      "softmax alpha: [0.12467811 0.12635206 0.12496297 0.12411094 0.12505319 0.12475361\n",
      " 0.12499079 0.12509834]\n",
      "==================================================\n",
      "60 600\n",
      "alpha:         [0.11127846 0.0792282  0.05663982 0.11425498 0.06299262]\n",
      "softmax alpha: [0.20529128 0.19881596 0.19437538 0.20590324 0.19561413]\n",
      "==================================================\n",
      "61 644\n",
      "alpha:         [0.1481298  0.09608144 0.14294107 0.10774429 0.08431646]\n",
      "softmax alpha: [0.20649608 0.19602321 0.2054274  0.19832279 0.19373052]\n",
      "==================================================\n",
      "62 646\n",
      "alpha:         [0.04271171 0.08049428 0.101768   0.05991974 0.07457325]\n",
      "softmax alpha: [0.19420976 0.20168788 0.2060245  0.19758065 0.20049721]\n",
      "==================================================\n",
      "63 664\n",
      "alpha:         [0.0016348  0.00142555 0.00122027 0.00075382 0.00268959 0.00267684\n",
      " 0.00264668 0.00274291 0.00314393 0.0008521  0.00171848 0.00110136\n",
      " 0.00185006]\n",
      "softmax alpha: [0.0769041  0.07688801 0.07687222 0.07683638 0.07698526 0.07698428\n",
      " 0.07698195 0.07698936 0.07702024 0.07684393 0.07691053 0.07686308\n",
      " 0.07692065]\n",
      "==================================================\n",
      "64 689\n",
      "alpha:         [0.02037788 0.01013835 0.00988823 0.02334086 0.00990453 0.01030127\n",
      " 0.00900722 0.00926398 0.00369816]\n",
      "softmax alpha: [0.11206992 0.11092824 0.1109005  0.11240248 0.1109023  0.11094631\n",
      " 0.11080283 0.11083129 0.11021613]\n",
      "==================================================\n",
      "65 696\n",
      "alpha:         [0.01194689 0.0155706  0.00847531 0.01239157 0.02022703 0.01620165\n",
      " 0.01012612 0.01930505 0.01026961]\n",
      "softmax alpha: [0.11090067 0.11130327 0.11051634 0.11095    0.11182275 0.11137353\n",
      " 0.11069893 0.1117197  0.11071481]\n",
      "==================================================\n",
      "66 704\n",
      "alpha:         [0.01233026 0.00335773 0.00480545 0.01178376 0.00529527 0.0077289\n",
      " 0.02854846 0.0042465  0.00338944 0.00959394]\n",
      "softmax alpha: [0.10032013 0.09942403 0.09956807 0.10026532 0.09961685 0.09985958\n",
      " 0.10196041 0.09951243 0.09942718 0.100046  ]\n",
      "==================================================\n",
      "67 727\n",
      "alpha:         [0.0697291  0.10128949 0.06731146 0.09137279 0.0698699 ]\n",
      "softmax alpha: [0.19795436 0.20430151 0.19747636 0.20228553 0.19798224]\n",
      "==================================================\n",
      "68 735\n",
      "alpha:         [0.12107605 0.10444529 0.15108229 0.2267041  0.26819765]\n",
      "softmax alpha: [0.18925519 0.18613376 0.19502009 0.21033981 0.21925116]\n",
      "==================================================\n",
      "69 740\n",
      "alpha:         [0.07349274 0.09323687 0.11470473 0.06759175 0.08968722]\n",
      "softmax alpha: [0.19714316 0.20107426 0.20543756 0.19598324 0.20036178]\n",
      "==================================================\n",
      "70 741\n",
      "alpha:         [0.01806755 0.02178087 0.02383315 0.02974255 0.02628086 0.02607618\n",
      " 0.09188776]\n",
      "softmax alpha: [0.14056499 0.14108792 0.14137777 0.1422157  0.14172425 0.14169524\n",
      " 0.15133413]\n",
      "==================================================\n",
      "71 747\n",
      "alpha:         [0.01094316 0.00540055 0.02762837 0.00734815 0.0064855  0.00509425\n",
      " 0.01576883 0.01009308 0.01644582]\n",
      "softmax alpha: [0.11102556 0.11041189 0.11289358 0.11062714 0.11053174 0.11037807\n",
      " 0.11156262 0.11093122 0.11163818]\n",
      "==================================================\n",
      "72 758\n",
      "alpha:         [0.00196458 0.00951533 0.00194609 0.00745733 0.00643013 0.00334198\n",
      " 0.00615488 0.00529405 0.00781758 0.01322082]\n",
      "softmax alpha: [0.09956543 0.10032007 0.09956359 0.10011382 0.10001104 0.09970267\n",
      " 0.09998351 0.09989748 0.1001499  0.10069249]\n",
      "==================================================\n",
      "73 775\n",
      "alpha:         [0.12338702 0.18133523 0.07029175 0.15457857 0.06484054]\n",
      "softmax alpha: [0.20069169 0.21266498 0.19031386 0.20705023 0.18927924]\n",
      "==================================================\n",
      "74 777\n",
      "alpha:         [0.03252912 0.01840225 0.01796337 0.01578739 0.01415121 0.02364252\n",
      " 0.00944331 0.01560869]\n",
      "softmax alpha: [0.12677078 0.1249925  0.12493765 0.12466609 0.12446228 0.12564921\n",
      " 0.1238777  0.12464381]\n",
      "==================================================\n",
      "75 778\n",
      "alpha:         [0.03755409 0.07453649 0.03137642 0.08683207 0.08396668]\n",
      "softmax alpha: [0.19494945 0.20229412 0.19374882 0.2047968  0.20421081]\n",
      "==================================================\n",
      "76 781\n",
      "alpha:         [0.0821273  0.04338612 0.09261776 0.05061304 0.08986687]\n",
      "softmax alpha: [0.2020491  0.19437116 0.20417984 0.19578096 0.20361894]\n",
      "==================================================\n",
      "77 788\n",
      "alpha:         [0.02976531 0.14330984 0.06488829 0.13342544 0.09199129 0.06899186]\n",
      "softmax alpha: [0.15700014 0.17587811 0.16261244 0.17414823 0.16707999 0.1632811 ]\n",
      "==================================================\n",
      "78 810\n",
      "alpha:         [0.0593343  0.10647497 0.11310063 0.05369439 0.08483948]\n",
      "softmax alpha: [0.19517082 0.20459161 0.20595167 0.19407318 0.20021272]\n",
      "==================================================\n",
      "79 817\n",
      "alpha:         [0.0020121  0.00153548 0.01674258 0.00143333 0.00178414 0.00263893\n",
      " 0.00134836 0.00140112 0.00134295 0.00123351 0.00130068 0.00116582\n",
      " 0.00173449]\n",
      "softmax alpha: [0.07686615 0.07682953 0.07800681 0.07682168 0.07684863 0.07691435\n",
      " 0.07681515 0.0768192  0.07681474 0.07680633 0.07681149 0.07680113\n",
      " 0.07684482]\n",
      "==================================================\n",
      "80 821\n",
      "alpha:         [0.100567   0.07604363 0.21327046 0.02647176 0.02295979]\n",
      "softmax alpha: [0.2020616  0.19716663 0.22616754 0.18763101 0.18697322]\n",
      "==================================================\n",
      "81 859\n",
      "alpha:         [0.20284131 0.21158514 0.05643189 0.21298721 0.08941465]\n",
      "softmax alpha: [0.20939982 0.2112388  0.18088039 0.21153518 0.18694581]\n",
      "==================================================\n",
      "82 864\n",
      "alpha:         [0.0696002  0.06196411 0.06857983 0.04351017 0.05302438 0.03237932]\n",
      "softmax alpha: [0.16912905 0.16784249 0.16895657 0.16477354 0.16634871 0.16294964]\n",
      "==================================================\n",
      "83 865\n",
      "alpha:         [0.07304622 0.0541201  0.10104253 0.09473998 0.01478411 0.05744048]\n",
      "softmax alpha: [0.16779952 0.16465359 0.17256367 0.1714795  0.15830251 0.16520121]\n",
      "==================================================\n",
      "84 877\n",
      "alpha:         [0.05902881 0.03799245 0.1679728  0.13941443 0.04115465]\n",
      "softmax alpha: [0.19378821 0.18975419 0.21609321 0.21000922 0.19035518]\n",
      "==================================================\n",
      "85 919\n",
      "alpha:         [0.00895239 0.02024562 0.01389082 0.00519867 0.00796496 0.00767823\n",
      " 0.01129554 0.00704132 0.00874997]\n",
      "softmax alpha: [0.1109812  0.11224164 0.11153063 0.11056539 0.11087167 0.11083988\n",
      " 0.11124155 0.11076931 0.11095874]\n",
      "==================================================\n",
      "86 928\n",
      "alpha:         [0.08807817 0.15155455 0.04969144 0.09099597 0.03711304]\n",
      "softmax alpha: [0.20075851 0.21391508 0.19319809 0.20134514 0.19078318]\n",
      "==================================================\n",
      "87 939\n",
      "alpha:         [0.08770382 0.14409497 0.14928235 0.01827362 0.0180398 ]\n",
      "softmax alpha: [0.20051501 0.21214717 0.21325052 0.18706551 0.18702178]\n",
      "==================================================\n",
      "88 940\n",
      "alpha:         [0.13122254 0.0547838  0.20966272 0.15701897 0.07130183]\n",
      "softmax alpha: [0.20096546 0.18617634 0.21736397 0.20621709 0.18927715]\n",
      "==================================================\n",
      "89 946\n",
      "alpha:         [0.02315402 0.01182532 0.00641609 0.00579261 0.00447837 0.01390949\n",
      " 0.02273219 0.00884154 0.01170518]\n",
      "softmax alpha: [0.11234434 0.11107881 0.11047958 0.11041072 0.11026571 0.11131056\n",
      " 0.11229696 0.11074787 0.11106546]\n",
      "==================================================\n",
      "90 958\n",
      "alpha:         [0.02149381 0.04300727 0.02883794 0.05861799 0.03803783 0.07706184\n",
      " 0.04052334]\n",
      "softmax alpha: [0.13966531 0.14270255 0.14069481 0.14494772 0.14199516 0.14764592\n",
      " 0.14234853]\n",
      "==================================================\n",
      "91 1010\n",
      "alpha:         [0.02408118 0.0762951  0.03708187 0.10242513 0.10053989 0.056916  ]\n",
      "softmax alpha: [0.15971842 0.1682795  0.16180842 0.1727346  0.17240926 0.16504979]\n",
      "==================================================\n",
      "92 1022\n",
      "alpha:         [0.01547573 0.02747691 0.03134118 0.01578164 0.01288102 0.03347671\n",
      " 0.0346908  0.01550771]\n",
      "softmax alpha: [0.12401752 0.12551484 0.1260008  0.12405546 0.12369615 0.12627017\n",
      " 0.12642357 0.12402149]\n",
      "==================================================\n",
      "93 1034\n",
      "alpha:         [0.02883152 0.0398961  0.03765533 0.0222209  0.03078704 0.02489446\n",
      " 0.02512341]\n",
      "softmax alpha: [0.14269962 0.1442873  0.14396435 0.1417594  0.14297895 0.14213891\n",
      " 0.14217146]\n",
      "==================================================\n",
      "94 1043\n",
      "alpha:         [0.06752735 0.03522786 0.01353906 0.04967433 0.03521851 0.02418048\n",
      " 0.02251407]\n",
      "softmax alpha: [0.14749835 0.14281035 0.13974631 0.14488843 0.14280902 0.14124135\n",
      " 0.14100618]\n",
      "==================================================\n",
      "95 1083\n",
      "alpha:         [0.01005732 0.00713474 0.00858584 0.02366759 0.01234152 0.02851699\n",
      " 0.01290884 0.02193574]\n",
      "softmax alpha: [0.12430023 0.12393748 0.12411746 0.12600356 0.12458448 0.12661608\n",
      " 0.12465518 0.12578553]\n",
      "==================================================\n",
      "96 1093\n",
      "alpha:         [0.05304233 0.02713022 0.02420281 0.10833936 0.09290488 0.0494143 ]\n",
      "softmax alpha: [0.16556582 0.16133077 0.16085918 0.17497898 0.17229901 0.16496623]\n",
      "==================================================\n",
      "97 1098\n",
      "alpha:         [0.02115697 0.03925198 0.04090886 0.02655594 0.0789276  0.07657134\n",
      " 0.05171739]\n",
      "softmax alpha: [0.1390607  0.14159991 0.14183472 0.13981352 0.14733091 0.14698417\n",
      " 0.14337606]\n",
      "==================================================\n",
      "98 1103\n",
      "alpha:         [0.05350841 0.02345297 0.05772591 0.06084376 0.04577636 0.06931952]\n",
      "softmax alpha: [0.16693892 0.16199614 0.16764447 0.16816797 0.16565311 0.16959938]\n",
      "==================================================\n",
      "99 1116\n",
      "alpha:         [0.01960249 0.0175453  0.00821993 0.00663872 0.00905697 0.01934984\n",
      " 0.00985373 0.01082985 0.01368238]\n",
      "softmax alpha: [0.11187352 0.11164361 0.11060733 0.11043258 0.11069995 0.11184526\n",
      " 0.11078819 0.11089639 0.11121317]\n",
      "==================================================\n",
      "100 1130\n",
      "alpha:         [0.07439214 0.14648025 0.11236134 0.09107238 0.07391101]\n",
      "softmax alpha: [0.19493981 0.20951157 0.20248384 0.19821873 0.19484605]\n",
      "==================================================\n",
      "101 1133\n",
      "alpha:         [0.06508156 0.08675651 0.18329254 0.13023212 0.16774533]\n",
      "softmax alpha: [0.18786993 0.19198645 0.21144414 0.20051728 0.2081822 ]\n",
      "==================================================\n",
      "102 1140\n",
      "alpha:         [0.03571526 0.03790731 0.06665667 0.02205941 0.05624699 0.05958223]\n",
      "softmax alpha: [0.1648817  0.16524352 0.17006312 0.1626454  0.168302   0.16886426]\n",
      "==================================================\n",
      "103 1149\n",
      "alpha:         [0.09701082 0.05227415 0.10293279 0.08881418 0.09418464]\n",
      "softmax alpha: [0.2019711  0.19313471 0.20317071 0.20032238 0.2014011 ]\n",
      "==================================================\n",
      "104 1161\n",
      "alpha:         [0.03159354 0.06711224 0.02941156 0.02908466 0.05298629 0.02362301\n",
      " 0.02652714]\n",
      "softmax alpha: [0.14204339 0.14717925 0.14173379 0.14168746 0.14511482 0.14091573\n",
      " 0.14132556]\n",
      "==================================================\n",
      "105 1182\n",
      "alpha:         [0.02117725 0.02571874 0.02346052 0.02209088 0.02022757 0.00458257\n",
      " 0.0084817  0.02056773]\n",
      "softmax alpha: [0.12535852 0.12592913 0.12564508 0.12547311 0.12523953 0.1232954\n",
      " 0.12377709 0.12528214]\n",
      "==================================================\n",
      "106 1195\n",
      "alpha:         [0.21832864 0.05181181 0.14176014 0.08846737 0.12934283]\n",
      "softmax alpha: [0.21901133 0.18541687 0.20286789 0.19233953 0.20036439]\n",
      "==================================================\n",
      "107 1197\n",
      "alpha:         [0.16000927 0.01666841 0.08086163 0.09571883 0.14771656]\n",
      "softmax alpha: [0.21204962 0.18373223 0.19591339 0.19884584 0.20945891]\n",
      "==================================================\n",
      "108 1206\n",
      "alpha:         [0.01382699 0.00980559 0.00620596 0.00666265 0.00648901 0.00512956\n",
      " 0.00496813 0.00834422 0.00750685]\n",
      "softmax alpha: [0.11179808 0.1113494  0.1109493  0.11099998 0.11098071 0.11082994\n",
      " 0.11081205 0.1111868  0.11109373]\n",
      "==================================================\n",
      "109 1209\n",
      "alpha:         [0.03826678 0.09892683 0.12042635 0.0733314  0.08247076]\n",
      "softmax alpha: [0.19123937 0.20319903 0.20761501 0.19806406 0.19988253]\n",
      "==================================================\n",
      "110 1220\n",
      "alpha:         [0.00263108 0.00157846 0.00116345 0.00120042 0.00106658 0.00085658\n",
      " 0.00075142 0.00139468 0.00103361 0.00265612 0.00115386 0.00122078\n",
      " 0.00169906]\n",
      "softmax alpha: [0.0770166  0.07693557 0.07690365 0.07690649 0.0768962  0.07688006\n",
      " 0.07687197 0.07692144 0.07689367 0.07701853 0.07690291 0.07690806\n",
      " 0.07694485]\n",
      "==================================================\n",
      "111 1221\n",
      "alpha:         [0.05512916 0.04915305 0.09820734 0.04992319 0.07246131 0.14219658]\n",
      "softmax alpha: [0.16283138 0.16186119 0.16999914 0.16198589 0.1656782  0.17764419]\n",
      "==================================================\n",
      "112 1232\n",
      "alpha:         [0.06942862 0.08398469 0.07905845 0.02712012 0.13380684]\n",
      "softmax alpha: [0.198043   0.20094681 0.19995933 0.18983887 0.211212  ]\n",
      "==================================================\n",
      "113 1236\n",
      "alpha:         [0.11537956 0.06827451 0.09201067 0.11999963 0.10172046]\n",
      "softmax alpha: [0.20317135 0.19382286 0.19847851 0.20411219 0.20041508]\n",
      "==================================================\n",
      "114 1247\n",
      "alpha:         [0.08754383 0.11013826 0.09872118 0.07178532 0.02850967 0.0495221 ]\n",
      "softmax alpha: [0.1688099  0.17266748 0.17070733 0.16617056 0.1591328  0.16251194]\n",
      "==================================================\n",
      "115 1266\n",
      "alpha:         [0.03974687 0.08009225 0.03690142 0.15060233 0.13468662 0.1000738 ]\n",
      "softmax alpha: [0.15829451 0.16481154 0.15784473 0.17685191 0.17405947 0.16813785]\n",
      "==================================================\n",
      "116 1285\n",
      "alpha:         [0.11277149 0.15667261 0.11008928 0.1627661  0.02484615]\n",
      "softmax alpha: [0.19962889 0.20858804 0.19909416 0.20986295 0.18282597]\n",
      "==================================================\n",
      "117 1287\n",
      "alpha:         [0.23198643 0.141845   0.22533826 0.15915205 0.23269185]\n",
      "softmax alpha: [0.20671252 0.1888943  0.20534282 0.19219196 0.20685839]\n",
      "==================================================\n",
      "118 1300\n",
      "alpha:         [0.03027412 0.05891705 0.08555254 0.08316218 0.03695142 0.07157286]\n",
      "softmax alpha: [0.16157525 0.16627016 0.17075835 0.17035067 0.16265775 0.16838782]\n",
      "==================================================\n",
      "119 1301\n",
      "alpha:         [0.00121944 0.00215687 0.00143304 0.00318325 0.00253552 0.00292774\n",
      " 0.00130499 0.0014722  0.00183607 0.00077855 0.00275068 0.00139621\n",
      " 0.00171327]\n",
      "softmax alpha: [0.07687068 0.07694277 0.0768871  0.07702179 0.07697191 0.07700211\n",
      " 0.07687725 0.07689011 0.07691809 0.07683679 0.07698848 0.07688427\n",
      " 0.07690865]\n",
      "==================================================\n",
      "120 1309\n",
      "alpha:         [0.06534767 0.03843035 0.0650956  0.083682   0.0866022  0.07120093]\n",
      "softmax alpha: [0.1661393  0.16172692 0.16609742 0.16921345 0.1697083  0.16711461]\n",
      "==================================================\n",
      "121 1310\n",
      "alpha:         [0.12056266 0.07459593 0.07826408 0.22143271 0.01751947]\n",
      "softmax alpha: [0.20317447 0.1940466  0.1947597  0.22473797 0.18328126]\n",
      "==================================================\n",
      "122 1316\n",
      "alpha:         [0.06704643 0.08400637 0.08404111 0.05191292 0.04715148 0.04950192]\n",
      "softmax alpha: [0.1671644  0.17002367 0.17002958 0.16465366 0.16387153 0.16425716]\n",
      "==================================================\n",
      "123 1327\n",
      "alpha:         [0.05579672 0.00784927 0.05205963 0.08324741 0.07368386 0.0624476\n",
      " 0.02441389]\n",
      "softmax alpha: [0.14344917 0.13673344 0.14291409 0.1474415  0.14603815 0.14440642\n",
      " 0.13901724]\n",
      "==================================================\n",
      "124 1330\n",
      "alpha:         [0.07932697 0.15919157 0.09201596 0.13093392 0.07788524]\n",
      "softmax alpha: [0.19427165 0.21042348 0.19675247 0.20456063 0.19399177]\n",
      "==================================================\n",
      "125 1342\n",
      "alpha:         [0.00968499 0.01747484 0.01874848 0.02432589 0.01014628 0.02564168\n",
      " 0.01489204 0.01729322]\n",
      "softmax alpha: [0.1240529  0.12502302 0.12518236 0.1258825  0.12411013 0.12604825\n",
      " 0.12470053 0.12500032]\n",
      "==================================================\n",
      "126 1354\n",
      "alpha:         [0.00046243 0.00095672 0.0011253  0.00059998 0.00046808 0.00032579\n",
      " 0.00089706 0.00116061 0.00101017 0.00066178 0.00035266 0.00121269\n",
      " 0.00242101 0.00045132 0.00027999]\n",
      "softmax alpha: [0.06664244 0.06667539 0.06668663 0.06665161 0.06664282 0.06663334\n",
      " 0.06667141 0.06668899 0.06667896 0.06665573 0.06663513 0.06669246\n",
      " 0.0667731  0.0666417  0.06663029]\n",
      "==================================================\n",
      "127 1372\n",
      "alpha:         [0.1223602  0.0725987  0.03418806 0.01904228 0.01551136 0.11771658]\n",
      "softmax alpha: [0.17658708 0.16801489 0.1616837  0.15925333 0.15869201 0.17576898]\n",
      "==================================================\n",
      "128 1385\n",
      "alpha:         [0.08348721 0.10929233 0.18425368 0.06931736 0.12978337]\n",
      "softmax alpha: [0.19359308 0.19865379 0.21411749 0.19086924 0.2027664 ]\n",
      "==================================================\n",
      "129 1393\n",
      "alpha:         [0.10789687 0.08812472 0.20199817 0.03897242 0.09446387]\n",
      "softmax alpha: [0.20003412 0.19611786 0.2197717  0.18671129 0.19736503]\n",
      "==================================================\n",
      "130 1399\n",
      "alpha:         [0.04271842 0.12985565 0.11961197 0.10256587 0.07079624]\n",
      "softmax alpha: [0.19007346 0.20737896 0.20526548 0.20179616 0.19548594]\n",
      "==================================================\n",
      "131 1402\n",
      "alpha:         [0.04075035 0.07910625 0.04519325 0.05163583 0.07634903 0.06066649]\n",
      "softmax alpha: [0.16364324 0.17004185 0.16437191 0.1654343  0.16957365 0.16693505]\n",
      "==================================================\n",
      "132 1409\n",
      "alpha:         [0.05140968 0.07290634 0.06764775 0.08388399 0.06084914 0.08704706]\n",
      "softmax alpha: [0.16348226 0.16703463 0.16615857 0.16887838 0.16503275 0.1694134 ]\n",
      "==================================================\n",
      "133 1429\n",
      "alpha:         [0.03588587 0.07395467 0.08481703 0.07146287 0.06390491 0.07490679]\n",
      "softmax alpha: [0.16146284 0.16772803 0.16955988 0.1673106  0.16605084 0.1678878 ]\n",
      "==================================================\n",
      "134 1436\n",
      "alpha:         [0.0345429  0.04802817 0.0474544  0.01740095 0.03349564 0.03809892\n",
      " 0.01570316]\n",
      "softmax alpha: [0.14299139 0.14493272 0.14484959 0.14056112 0.14284172 0.14350077\n",
      " 0.14032268]\n",
      "==================================================\n",
      "135 1437\n",
      "alpha:         [0.11827166 0.14315838 0.11733103 0.19749001 0.08678859]\n",
      "softmax alpha: [0.1970169  0.20198152 0.19683166 0.2132591  0.19091082]\n",
      "==================================================\n",
      "136 1442\n",
      "alpha:         [0.17341369 0.15989807 0.15696016 0.07342913 0.05368703]\n",
      "softmax alpha: [0.20998386 0.20716489 0.20655716 0.19000419 0.18628989]\n",
      "==================================================\n",
      "137 1466\n",
      "alpha:         [0.05800157 0.12819566 0.0472365  0.0862947  0.09188538]\n",
      "softmax alpha: [0.19511546 0.20930354 0.19302629 0.20071472 0.20183999]\n",
      "==================================================\n",
      "138 1470\n",
      "alpha:         [0.06333302 0.22096585 0.11083063 0.13852178 0.04570883]\n",
      "softmax alpha: [0.18939402 0.22173044 0.19860684 0.20418335 0.18608534]\n",
      "==================================================\n",
      "139 1493\n",
      "alpha:         [0.09915685 0.10012149 0.12162454 0.14598969 0.13618434]\n",
      "softmax alpha: [0.1957194  0.19590829 0.20016654 0.20510352 0.20310224]\n",
      "==================================================\n",
      "140 1494\n",
      "alpha:         [0.11743632 0.02904943 0.10585271 0.02171907 0.0302817 ]\n",
      "softmax alpha: [0.21145466 0.193567   0.20901938 0.19215327 0.19380568]\n",
      "==================================================\n",
      "141 1508\n",
      "alpha:         [0.00296589 0.00512916 0.0055209  0.0017206  0.00598893 0.00374567\n",
      " 0.00391697 0.00258707 0.00150537 0.00207304]\n",
      "softmax alpha: [0.09994495 0.10016139 0.10020064 0.09982057 0.10024755 0.10002292\n",
      " 0.10004005 0.0999071  0.09979909 0.09985575]\n",
      "==================================================\n",
      "142 1516\n",
      "alpha:         [0.01584723 0.03505446 0.020365   0.03353517 0.03388609 0.01462448\n",
      " 0.01295835 0.01984754]\n",
      "softmax alpha: [0.12407148 0.12647758 0.12463327 0.12628557 0.12632989 0.12391986\n",
      " 0.12371356 0.12456879]\n",
      "==================================================\n",
      "143 1518\n",
      "alpha:         [0.05768502 0.04786372 0.13034449 0.0544944  0.09839427 0.04410367]\n",
      "softmax alpha: [0.16419107 0.16258639 0.17656521 0.16366803 0.17101308 0.16197621]\n",
      "==================================================\n",
      "144 1525\n",
      "alpha:         [0.00910603 0.01994926 0.00895635 0.02028414 0.01751775 0.01421336\n",
      " 0.0015612  0.01462993 0.02316544]\n",
      "softmax alpha: [0.11052481 0.11172978 0.11050827 0.1117672  0.11145843 0.11109074\n",
      " 0.10969406 0.11113703 0.1120897 ]\n",
      "==================================================\n",
      "145 1529\n",
      "alpha:         [0.10095238 0.08438067 0.07167118 0.17227633 0.12414861]\n",
      "softmax alpha: [0.19793731 0.19468418 0.1922255  0.21257063 0.20258238]\n",
      "==================================================\n",
      "146 1547\n",
      "alpha:         [0.00250496 0.0019335  0.0044875  0.00277445 0.00357531 0.00184641\n",
      " 0.00220927 0.00370191 0.00254905 0.00267491 0.00298966]\n",
      "softmax alpha: [0.09087855 0.09082664 0.0910589  0.09090305 0.09097588 0.09081873\n",
      " 0.09085169 0.0909874  0.09088256 0.090894   0.09092261]\n",
      "==================================================\n",
      "147 1554\n",
      "alpha:         [0.05186718 0.06728805 0.02443443 0.10056294 0.1437661 ]\n",
      "softmax alpha: [0.1947558  0.19778238 0.18948573 0.20447428 0.21350182]\n",
      "==================================================\n",
      "148 1563\n",
      "alpha:         [0.01630026 0.01606791 0.01058356 0.01799177 0.02319203 0.02070049\n",
      " 0.02102521 0.0109295  0.01062909]\n",
      "softmax alpha: [0.11110111 0.1110753  0.11046779 0.1112892  0.11186944 0.11159105\n",
      " 0.1116273  0.11050601 0.11047282]\n",
      "==================================================\n",
      "149 1573\n",
      "alpha:         [0.06061667 0.05208829 0.0529096  0.0861922  0.05907488 0.04712646]\n",
      "softmax alpha: [0.16681136 0.16539478 0.16553067 0.17113267 0.16655437 0.16457615]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "\n",
    "#with Embedding\n",
    "result = np.zeros((usr_test_amount, movie_nb))\n",
    "RS = np.zeros((usr_test_amount, movie_nb))\n",
    "\n",
    "#test_idx --> Test 的 index length = 150\n",
    "test_yes_id = []\n",
    "\n",
    "for s in range(usr_test_amount):\n",
    "    print(s, test_idx[s])\n",
    "\n",
    "    yes = []\n",
    "    sample = random.sample(train_t[test_idx[s]],len(train_t[test_idx[s]]))\n",
    "    #sample=result_yes_id[now]\n",
    "    test_yes_id.append(sample)\n",
    "    alpha = np.zeros([len(sample)])\n",
    "    \n",
    "    for a in range(len(sample)):\n",
    "        r = np.max(movie_genre[sample[a]] * usr_genre_norm[test_idx[s]]) #sample a 的category vec *user_category vec\n",
    "        \n",
    "# #         ''' Observe each part in attention\n",
    "#         WuUu = np.sum(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T))\n",
    "#         WyYy = np.sum(np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T))\n",
    "#         WaAa = np.sum(np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T))\n",
    "#         WvVy = np.sum(np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))\n",
    "#         print('The sum of each par -->',\n",
    "#               '\\nw1:',testW1,\n",
    "#               '\\nWuU:',WuUu,\n",
    "#               '\\nwyY:',WyYy,\n",
    "#               '\\nWaA:',WaAa,\n",
    "#               '\\nWvV:',WvVy)\n",
    "# #         '''\n",
    "        \n",
    "        alpha[a] = np.sum((relu(np.dot(Au[test_idx[s]],np.expand_dims(U[test_idx[s]],0).T) +\n",
    "                                np.dot(Ay[sample[a]],np.expand_dims(Y[sample[a]],0).T) +\n",
    "                                np.dot(Aa[test_idx[s]],np.expand_dims(A[sample[a]],0).T) +\n",
    "                                np.dot(np.dot(Av[test_idx[s]], E),np.expand_dims(all_npy[sample[a]],0).T))))*r\n",
    "        \n",
    "    mul = np.zeros((1,latent_dim))\n",
    "    \n",
    "    print(\"{:<15}{}\".format('alpha:', alpha))\n",
    "    print(\"{:<15}{}\".format('softmax alpha:', softmax(alpha)))\n",
    "    print('==================================================')\n",
    "    \n",
    "    for i in range(len(sample)):\n",
    "        mul += alpha[i] * A[sample[i]] #attention alpha * Ai part \n",
    "    new_mul = mul + U[test_idx[s]]  #(U+auxilary)\n",
    "    \n",
    "    for k in range(movie_nb):\n",
    "        result[s][k] = np.dot(new_mul,Y[k].T) #(U+auxilary)*photo latent factor\n",
    "        RS[s][k] = np.dot(new_mul,Y[k].T) + np.dot(B[test_idx[s]], np.dot(E, all_npy[k].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 165)\n"
     ]
    }
   ],
   "source": [
    "#取出test的資料\n",
    "print(RS.shape)\n",
    "\n",
    "testRS = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "target = np.zeros((usr_test_amount, movie_test_amount)) #shape 150 * 32\n",
    "        \n",
    "for z in range(usr_test_amount):\n",
    "    user_id = test_idx[z]\n",
    "    # positive target YouTuber list\n",
    "    youtube_t = test_t[z] \n",
    "    # not target YouTuber list\n",
    "    youtube_f = test_f[z]\n",
    "    \n",
    "#     print(user_id)\n",
    "#     print(youtube_t)\n",
    "#     print(youtube_f)\n",
    "    \n",
    "    #前面放target的RS\n",
    "    for i in range(len(youtube_t)):\n",
    "        testRS[z][i] = RS[z][youtube_t[i]]\n",
    "        target[z][i] = 1\n",
    "        \n",
    "    for i in range(len(youtube_f)):\n",
    "        testRS[z][i+len(youtube_t)] = RS[z][youtube_f[i]]\n",
    "    \n",
    "#     print(testRS[z])\n",
    "#     print(target[z])\n",
    "#     print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) (150, 32)\n",
      "num of positive data in testing: 1078.0\n"
     ]
    }
   ],
   "source": [
    "print(target.shape, testRS.shape)\n",
    "sumtarget = np.sum(target)\n",
    "print('num of positive data in testing:', sumtarget) # whole matrix: 4800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = (2*prec*rec)/(prec+rec)\n",
    "    return f1\n",
    "\n",
    "def topN(RSls, target, n):\n",
    "    maxn = np.argsort(RSls)[::-1][:n]\n",
    "    sum_target = int(np.sum(target))\n",
    "#     print(maxn, sum_target)\n",
    "    TP = 0\n",
    "    for i in maxn:\n",
    "        if i < sum_target:\n",
    "            TP += 1  \n",
    "    return TP, maxn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1\n",
      "Num of TP: 60\n",
      "prec: 0.4\n",
      "recall: 0.055658627087198514\n",
      "F1_score: 0.09771986970684039\n",
      "==============================\n",
      "Top 3\n",
      "Num of TP: 246\n",
      "prec: 0.5466666666666666\n",
      "recall: 0.22820037105751392\n",
      "F1_score: 0.32198952879581155\n",
      "==============================\n",
      "Top 5\n",
      "Num of TP: 535\n",
      "prec: 0.7133333333333334\n",
      "recall: 0.4962894248608534\n",
      "F1_score: 0.5853391684901532\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "N = [1, 3, 5]\n",
    "correct = 0\n",
    "\n",
    "for n in N:\n",
    "    print('Top', n)\n",
    "    \n",
    "    for i in range(len(testRS)):\n",
    "        TP, _ = topN(testRS[i], target[i], n)\n",
    "        correct += TP\n",
    "\n",
    "    print('Num of TP:', correct)\n",
    "\n",
    "    prec = correct/(len(testRS)*n)\n",
    "    recall = correct/sumtarget\n",
    "    \n",
    "    print('prec:', prec)\n",
    "    print('recall:', recall)\n",
    "    print('F1_score:', F1_score(prec, recall))\n",
    "    \n",
    "    print('==============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "usr_test_amount = 150\n",
    "movie_test_amount = 32\n",
    "'''\n",
    "all_sort = []\n",
    "pre_matrix = np.zeros(shape=(usr_test_amount, movie_test_amount))\n",
    "\n",
    "for i in range(usr_test_amount):\n",
    "    TP, top_n = topN(list(testRS[i]), target[i],len(testRS[i]))\n",
    "#     print(top_n)\n",
    "    all_sort.append(top_n)\n",
    "#     print(all_sort)\n",
    "    \n",
    "    TP, top_5 = topN(list(testRS[i]), target[i], 5)\n",
    "    for j in range(len(top_5)):\n",
    "        pre_matrix[i][top_n[j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32) 150\n"
     ]
    }
   ],
   "source": [
    "print(pre_matrix.shape, len(all_sort))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDCG\n",
    "* https://daiwk.github.io/posts/nlp-ndcg.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Ideal DCG，理想状况下的DCG。也就是说，相关性完全由高到低排序时算出的DCG：\n",
    "\n",
    "def IDCG(ideal_list): #ideal_list example = [1,1,1,1,1,0,0,....]\n",
    "    idcg = 0\n",
    "    for i in range(len(ideal_list)):\n",
    "        idcg += (2**ideal_list[i]-1)/math.log2(i+2)\n",
    "    return idcg\n",
    "\n",
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.39388282130523067\n"
     ]
    }
   ],
   "source": [
    "total_ndcg = 0\n",
    "num_ndcg = 5\n",
    "\n",
    "for m in range(usr_test_amount):\n",
    "    idcg = IDCG([1] * num_ndcg)\n",
    "#     print(idcg)\n",
    "    pre_list = []\n",
    "#     print(all_sort[m][:num_ndcg])\n",
    "\n",
    "    for s in all_sort[m][:num_ndcg]:\n",
    "        pre_list.append(target[m][s])\n",
    "        \n",
    "    dcg = DCG(pre_list)\n",
    "    ndcg = dcg/idcg\n",
    "    total_ndcg += ndcg\n",
    "    \n",
    "avg_ndcg = total_ndcg/usr_test_amount\n",
    "print('NDCG:',avg_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP: 0.30494540345275606\n"
     ]
    }
   ],
   "source": [
    "total_prec = 0\n",
    "for u in range(usr_test_amount):\n",
    "    y_true = target[u]\n",
    "    y_scores = pre_matrix[u]\n",
    "    total_prec += average_precision_score(y_true, y_scores)\n",
    "    \n",
    "MAP = total_prec/usr_test_amount\n",
    "\n",
    "print('MAP:', MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
