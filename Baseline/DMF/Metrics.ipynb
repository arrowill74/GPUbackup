{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os \n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJson(name):\n",
    "    with open(name+'/test_truth.json') as json_file:\n",
    "        test_truth = json.load(json_file)\n",
    "    with open(name+'/test_rank.json') as json_file:\n",
    "        test_rank = json.load(json_file)\n",
    "    with open(name+'/test_score.json') as json_file:\n",
    "        test_score = json.load(json_file)\n",
    "#         test_score = eval(test_score)\n",
    "    with open(name+'/test_item.json') as json_file:\n",
    "        test_item = json.load(json_file)\n",
    "        \n",
    "#     print(type(test_truth), type(test_rank), type(test_score), type(test_item)) # all dict\n",
    "#     print(len(test_truth), len(test_rank), len(test_score), len(test_item)) # all 150\n",
    "    \n",
    "    return test_truth,test_rank,test_score,test_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCG(prec_list): #找出前n名的[1,1,1,0,...]\n",
    "    dcg = 0\n",
    "    for i in range(len(prec_list)):\n",
    "        dcg += (2**prec_list[i]-1)/math.log2(i+2)\n",
    "    return dcg\n",
    "\n",
    "def NDCG(test_truth, test_rank, num_ndcg): #test_truth是每位User真正positive的item\n",
    "    total_ndcg = 0\n",
    "\n",
    "    for user in test_truth.keys():\n",
    "#         print('user:', user)\n",
    "        user_truth = test_truth[user]\n",
    "        user_rank = test_rank[user]\n",
    "#         print(len(user_truth), user_truth)\n",
    "#         print(len(user_rank), user_rank)\n",
    "        if len(user_truth) >= 32:\n",
    "            print('Error!!!')\n",
    "            break\n",
    "        \n",
    "        if not len(user_rank) == 32:\n",
    "            print('Error!!!')\n",
    "            break\n",
    "        \n",
    "        len_for_truth = len(user_truth)\n",
    "        target = []\n",
    "        for i in range(len(user_rank)):\n",
    "            if i < len_for_truth:\n",
    "                target.append(1)\n",
    "            else:\n",
    "                target.append(0)\n",
    "#         print(sum(target), target)\n",
    "        \n",
    "        if not sum(target) == len_for_truth:\n",
    "            print('Error!!!')\n",
    "            break\n",
    "        \n",
    "        idcg = DCG(target[:num_ndcg])\n",
    "        \n",
    "        pre_list = []\n",
    "        for item in user_rank[:num_ndcg]:\n",
    "            if item in user_truth:\n",
    "                pre_list.append(1)\n",
    "            else:\n",
    "                pre_list.append(0)\n",
    "#         print('pre_list:', pre_list)\n",
    "        if sum(pre_list) > len_for_truth:\n",
    "            print('Error!!!')\n",
    "            break\n",
    "            \n",
    "        dcg = DCG(pre_list)\n",
    "        ndcg = dcg/idcg\n",
    "        total_ndcg += ndcg\n",
    "#         print('==================================================')\n",
    "    avg_ndcg = total_ndcg/len(test_truth)\n",
    "#     print('len(test_truth):', len(test_truth))\n",
    "    return avg_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def MAP(test_item, test_truth, test_score):\n",
    "    Map_value = []\n",
    "    for user in test_item.keys():\n",
    "#         print('user:', user)\n",
    "        user_item = test_item[user]\n",
    "        user_truth = test_truth[user]\n",
    "#         print('user_item:', user_item)\n",
    "#         print('user_truth:', user_truth)\n",
    "        if not len(user_item) == 32:\n",
    "            print('Error!!!')\n",
    "            break\n",
    "        \n",
    "        y_true = np.zeros(len(user_item))\n",
    "        for tp in user_truth:\n",
    "            y_true[user_item.index(tp)] = 1\n",
    "#         print('y_true:', y_true)\n",
    "        \n",
    "        y_scores = test_score[user]\n",
    "        Map_value.append(average_precision_score(y_true, y_scores))\n",
    "    \n",
    "#     print('len(test_truth):', len(Map_value),len(test_truth))\n",
    "    return np.mean(Map_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1_score(prec,rec):\n",
    "    f1 = 2*((prec*rec)/(prec+rec))\n",
    "    return f1\n",
    "\n",
    "def topN(test_rank, test_truth, n):\n",
    "    print('Top', n)\n",
    "    sumtarget = 0\n",
    "    correct = 0\n",
    "    for user in test_rank.keys():\n",
    "#         print('user:', user)\n",
    "        user_rank = test_rank[user]\n",
    "        user_truth = test_truth[user]\n",
    "        if not len(user_rank) == 32:\n",
    "            print('Error!!!')\n",
    "            break\n",
    "            \n",
    "        pos = np.zeros(len(user_rank))\n",
    "        for tp in user_truth:\n",
    "            pos[user_rank.index(tp)] = 1\n",
    "#         print('pos:', pos)\n",
    "#         print('sum(pos[:n]):',sum(pos[:n]))\n",
    "        correct += sum(pos[:n])\n",
    "        sumtarget += len(user_truth)\n",
    "#         print('len(user_truth):', len(user_truth))\n",
    "\n",
    "    print('correct:', correct) \n",
    "    print('sumtarget:', sumtarget)\n",
    "    prec = correct/(len(test_rank)*n) #150*n\n",
    "    recall = correct/sumtarget\n",
    "\n",
    "    print('prec:', prec)\n",
    "    print('recall:', recall)\n",
    "    print('F1_score:', F1_score(prec, recall))\n",
    "    print('*****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Result_original\n",
      "NDCG@10: 0.5266850544076258\n",
      "MAP: 0.45775634149142663\n",
      "Top 1\n",
      "correct: 74.0\n",
      "sumtarget: 1078\n",
      "prec: 0.49333333333333335\n",
      "recall: 0.0686456400742115\n",
      "F1_score: 0.12052117263843648\n",
      "*****\n",
      "Top 5\n",
      "correct: 350.0\n",
      "sumtarget: 1078\n",
      "prec: 0.4666666666666667\n",
      "recall: 0.3246753246753247\n",
      "F1_score: 0.38293216630196936\n",
      "*****\n",
      "==================================================\n",
      "Name: Result_cmovie\n",
      "NDCG@10: 0.3508388530475079\n",
      "MAP: 0.3268097659768211\n",
      "Top 1\n",
      "correct: 24.0\n",
      "sumtarget: 561\n",
      "prec: 0.24\n",
      "recall: 0.0427807486631016\n",
      "F1_score: 0.07261724659606657\n",
      "*****\n",
      "Top 5\n",
      "correct: 133.0\n",
      "sumtarget: 561\n",
      "prec: 0.266\n",
      "recall: 0.23707664884135474\n",
      "F1_score: 0.2507068803016023\n",
      "*****\n",
      "==================================================\n",
      "Name: Result_cuser\n",
      "NDCG@10: 0.49075346338438147\n",
      "MAP: 0.4183901935427587\n",
      "Top 1\n",
      "correct: 49.0\n",
      "sumtarget: 772\n",
      "prec: 0.49\n",
      "recall: 0.06347150259067358\n",
      "F1_score: 0.11238532110091744\n",
      "*****\n",
      "Top 5\n",
      "correct: 208.0\n",
      "sumtarget: 772\n",
      "prec: 0.416\n",
      "recall: 0.2694300518134715\n",
      "F1_score: 0.3270440251572327\n",
      "*****\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "nameList = ['Result_original', 'Result_cmovie', 'Result_cuser']\n",
    "num_of_y_test = 32\n",
    "for name in nameList:\n",
    "    test_truth,test_rank,test_score,test_item = getJson(name)\n",
    "    print('Name:',name)\n",
    "    print('NDCG@10:', NDCG(test_truth, test_rank, 10))\n",
    "    print('MAP:', MAP(test_item, test_truth, test_score))\n",
    "    topN(test_rank, test_truth, 1)\n",
    "    topN(test_rank, test_truth, 5)\n",
    "    print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
