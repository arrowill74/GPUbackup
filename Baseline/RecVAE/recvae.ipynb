{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:24.147156Z",
     "start_time": "2019-12-24T14:59:22.684738Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:24.156079Z",
     "start_time": "2019-12-24T14:59:24.148473Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f23ec2f5850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1337\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:24.169763Z",
     "start_time": "2019-12-24T14:59:24.157790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:26.353143Z",
     "start_time": "2019-12-24T14:59:24.171374Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ml-20m/pro_sg/unique_sid.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-15d3060e13b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_indexing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pro_sg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_1_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_2_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_1_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_2_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/yuzen/Baseline/RecVAE/utils.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(global_indexing, dataset)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0munique_sid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpro_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unique_sid.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0munique_sid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ml-20m/pro_sg/unique_sid.txt'"
     ]
    }
   ],
   "source": [
    "data = (x.astype('float32') for x in utils.get_data(global_indexing=False, dataset='pro_sg'))\n",
    "train_data, valid_1_data, valid_2_data, test_1_data, test_2_data = data\n",
    "n_users, n_items = train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:26.554503Z",
     "start_time": "2019-12-24T14:59:26.354633Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fcccdae4ef48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrelative_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mser_model_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_notebook_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mser_model_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-fcccdae4ef48>\u001b[0m in \u001b[0;36mget_notebook_name\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murljoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'api/sessions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkernel_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mrelative_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'notebook'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrelative_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import ipykernel\n",
    "import requests\n",
    "from requests.compat import urljoin\n",
    "from notebook.notebookapp import list_running_servers\n",
    "\n",
    "def get_notebook_name():\n",
    "    kernel_id = re.search('kernel-(.*).json', ipykernel.connect.get_connection_file()).group(1)\n",
    "    servers = list_running_servers()\n",
    "    for ss in servers:\n",
    "        response = requests.get(urljoin(ss['url'], 'api/sessions'), params={'token': ss.get('token', '')})\n",
    "        for nn in json.loads(response.text):\n",
    "            if nn['kernel']['id'] == kernel_id:\n",
    "                relative_path = nn['notebook']['path']\n",
    "                return relative_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "ser_model_fn = 'model_' + get_notebook_name().replace(' ', '_') + '.pt'\n",
    "print(ser_model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:26.574453Z",
     "start_time": "2019-12-24T14:59:26.555956Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def swish_(x):\n",
    "    return x.mul_(torch.sigmoid(x))\n",
    "\n",
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def kl(q_distr, p_distr, weights, eps=1e-7):\n",
    "    mu_q, logvar_q = q_distr\n",
    "    mu_p, logvar_p = p_distr\n",
    "    return 0.5 * (((logvar_q.exp() + (mu_q - mu_p).pow(2)) / (logvar_p.exp() + eps) \\\n",
    "                    + logvar_p - logvar_q - 1\n",
    "                   ).sum(dim=-1) * weights).mean()\n",
    "\n",
    "def simple_kl(mu_q, logvar_q, logvar_p_scale, norm):\n",
    "    return (-0.5 * ( (1 + logvar_q #- torch.log(torch.ones(1)*logvar_p_scale) \\\n",
    "                      - mu_q.pow(2)/logvar_p_scale - logvar_q.exp()/logvar_p_scale\n",
    "                     )\n",
    "                   ).sum(dim=-1) * norm\n",
    "           ).mean()\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "def log_norm_std_pdf(x):\n",
    "    return -0.5*(np.log(2 * np.pi) + x.pow(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:26.610106Z",
     "start_time": "2019-12-24T14:59:26.575688Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeterministicDecoder(nn.Linear):\n",
    "    def __init__(self, *args):\n",
    "        super(DeterministicDecoder, self).__init__(*args)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        output = super(DeterministicDecoder, self).forward(*args)\n",
    "        return output, 0\n",
    "\n",
    "\n",
    "class StochasticDecoder(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(StochasticDecoder, self).__init__(in_features, out_features, bias)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.logvar = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.logvar.data.fill_(-2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        if self.training:\n",
    "            std = torch.exp(self.logvar)\n",
    "            a = F.linear(input, self.weight, self.bias)\n",
    "            eps = torch.randn_like(a)\n",
    "            b = eps.mul_(torch.sqrt_(F.linear(input * input, std)))\n",
    "            output = a + b\n",
    "            \n",
    "            kl = (-0.5 * (1 + self.logvar - self.weight.pow(2) - self.logvar.exp())).sum(dim=-1).mean() #/ (10)\n",
    "            return output, kl\n",
    "        else:\n",
    "            output = F.linear(input, self.weight, self.bias)\n",
    "            return output, 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:26.625042Z",
     "start_time": "2019-12-24T14:59:26.611468Z"
    }
   },
   "outputs": [],
   "source": [
    "class GaussianMixturePrior(nn.Module):\n",
    "    def __init__(self, latent_dim, gaussians_number):\n",
    "        super(GaussianMixturePrior, self).__init__()\n",
    "        \n",
    "        self.gaussians_number = gaussians_number\n",
    "        \n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(latent_dim, gaussians_number))\n",
    "        self.mu_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(latent_dim, gaussians_number))\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        density_per_gaussian = log_norm_pdf(x=z[:, :, None],\n",
    "                                            mu=self.mu_prior[None, ...].detach(),\n",
    "                                            logvar=self.logvar_prior[None, ...].detach()\n",
    "                                           ).add(-np.log(self.gaussians_number))\n",
    "        \n",
    "      \n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:26.662664Z",
     "start_time": "2019-12-24T14:59:26.626229Z"
    }
   },
   "outputs": [],
   "source": [
    "class GaussianMixturePriorWithAprPost(nn.Module):\n",
    "    def __init__(self, latent_dim, input_count):\n",
    "        super(GaussianMixturePriorWithAprPost, self).__init__()\n",
    "        \n",
    "        self.gaussians_number = 1\n",
    "        \n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(latent_dim, self.gaussians_number))\n",
    "        self.mu_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(latent_dim, self.gaussians_number))\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "        \n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(latent_dim, self.gaussians_number))\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "        \n",
    "        self.user_mu = nn.Embedding(input_count, latent_dim)\n",
    "        self.user_logvar = nn.Embedding(input_count, latent_dim)\n",
    "        \n",
    "    def forward(self, z, idx):\n",
    "        density_per_gaussian1 = log_norm_pdf(x=z[:, :, None],\n",
    "                                            mu=self.mu_prior[None, :, :].detach(),\n",
    "                                            logvar=self.logvar_prior[None, :, :].detach()\n",
    "                                           ).add(np.log(1/5 - 1/20))\n",
    "        \n",
    "        \n",
    "        density_per_gaussian2 = log_norm_pdf(x=z[:, :, None],\n",
    "                                            mu=self.user_mu(idx)[:, :, None].detach(),\n",
    "                                            logvar=self.user_logvar(idx)[:, :, None].detach()\n",
    "                                           ).add(np.log(4/5 - 1/20))\n",
    "        \n",
    "        density_per_gaussian3 = log_norm_pdf(x=z[:, :, None],\n",
    "                                            mu=self.mu_prior[None, :, :].detach(),\n",
    "                                            logvar=self.logvar_uniform_prior[None, :, :].detach()\n",
    "                                           ).add(np.log(1/10))\n",
    "        \n",
    "        density_per_gaussian = torch.cat([density_per_gaussian1,\n",
    "                                          density_per_gaussian2,\n",
    "                                          density_per_gaussian3], dim=-1)\n",
    "                \n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:26.985684Z",
     "start_time": "2019-12-24T14:59:26.663867Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, matrix_dim, axis):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(matrix_dim[1], hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.prior = GaussianMixturePriorWithAprPost(latent_dim, matrix_dim[0])\n",
    "        self.decoder = DeterministicDecoder(latent_dim, matrix_dim[1])\n",
    "        \n",
    "        self.axis = axis\n",
    "\n",
    "\n",
    "    def encode(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "    \n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "        \n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "        return self.fc21(h5), self.fc22(h5)\n",
    "    \n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, user_ratings, user_idx, beta=1, dropout_rate=0.5, calculate_loss=True, mode=None):\n",
    "        \n",
    "        if mode == 'pr':\n",
    "            mu, logvar = self.encode(user_ratings, dropout_rate=dropout_rate)\n",
    "        elif mode == 'mf':\n",
    "            mu, logvar = self.encode(user_ratings, dropout_rate=0)\n",
    "            \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_pred, decoder_loss = self.decode(z)\n",
    "        \n",
    "        NLL = -(F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "        \n",
    "        if calculate_loss:\n",
    "            if mode == 'pr':\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                KLD = -(self.prior(z, user_idx) - log_norm_pdf(z, mu, logvar)).sum(dim=-1).mul(norm).mean()\n",
    "                loss = NLL + beta * KLD + decoder_loss\n",
    "            \n",
    "            elif mode == 'mf':\n",
    "                KLD = NLL * 0\n",
    "                loss = NLL + decoder_loss\n",
    "            \n",
    "            return (NLL, KLD), loss\n",
    "            \n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def set_embeddings(self, train_data, momentum=0, weight=None):\n",
    "        istraining = self.training\n",
    "        self.eval()\n",
    "\n",
    "        for batch in generate(batch_size=500, device=device, data_1=train_data, axis=self.axis):\n",
    "\n",
    "            user_ratings = batch.get_ratings_to_dev()\n",
    "            users_idx = batch.get_idx()\n",
    "\n",
    "            new_user_mu, new_user_logvar = self.encode(user_ratings, 0)\n",
    "\n",
    "            old_user_mu = self.prior.user_mu.weight.data[users_idx,:].detach()\n",
    "            old_user_logvar = self.prior.user_logvar.weight.data[users_idx,:].detach()\n",
    "\n",
    "            if weight:\n",
    "                old_user_var = torch.exp(old_user_logvar)\n",
    "                new_user_var = torch.exp(new_user_logvar)\n",
    "\n",
    "                post_user_var = 1 / (1 / old_user_var + weight / new_user_var)\n",
    "                post_user_mu = (old_user_mu / old_user_var + weight * new_user_mu / new_user_var) * post_user_var\n",
    "\n",
    "                self.prior.user_mu.weight.data[users_idx,:] = post_user_mu\n",
    "                self.prior.user_logvar.weight.data[users_idx,:] = torch.log(post_user_var + new_user_var)\n",
    "            else:\n",
    "                self.prior.user_mu.weight.data[users_idx,:] = momentum * old_user_mu + (1-momentum) * new_user_mu\n",
    "                self.prior.user_logvar.weight.data[users_idx,:] = momentum * old_user_logvar + (1-momentum) * new_user_logvar\n",
    "\n",
    "        if istraining:\n",
    "            self.train()\n",
    "        else:\n",
    "            self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:27.036291Z",
     "start_time": "2019-12-24T14:59:26.987049Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def generate(batch_size, device, axis, data_1, data_2=None, shuffle=False, samples_perc_per_epoch=1):\n",
    "    assert axis in ['users', 'items']\n",
    "    assert 0 < samples_perc_per_epoch <= 1\n",
    "    \n",
    "    if axis == 'items':\n",
    "        data_1 = data_1.T\n",
    "        if data_2 is not None:\n",
    "            data_2 = data_2.T\n",
    "    \n",
    "    total_samples = data_1.shape[0]\n",
    "    samples_per_epoch = int(total_samples * samples_perc_per_epoch)\n",
    "    \n",
    "    \n",
    "    if shuffle:\n",
    "        idxlist = np.arange(total_samples)\n",
    "        np.random.shuffle(idxlist)\n",
    "        idxlist = idxlist[:samples_per_epoch]\n",
    "    else:\n",
    "        idxlist = np.arange(samples_per_epoch)\n",
    "    \n",
    "    for st_idx in tqdm(range(0, samples_per_epoch, batch_size)):\n",
    "        end_idx = min(st_idx + batch_size, samples_per_epoch)\n",
    "        idx = idxlist[st_idx:end_idx]\n",
    "\n",
    "        yield Batch(device, idx, data_1, data_2)\n",
    "\n",
    "\n",
    "class Batch:\n",
    "    def __init__(self, device, idx, data_1, data_2=None):\n",
    "        self._device = device\n",
    "        self._idx = idx\n",
    "        self._data_1 = data_1\n",
    "        self._data_2 = data_2\n",
    "    \n",
    "    def get_idx(self):\n",
    "        return self._idx\n",
    "    \n",
    "    def get_idx_to_dev(self):\n",
    "        return torch.LongTensor(self.get_idx()).to(self._device)\n",
    "        \n",
    "    def get_ratings(self, is_test=False):\n",
    "        data = self._data_2 if is_test else self._data_1\n",
    "        return data[self._idx]\n",
    "    \n",
    "    def get_ratings_to_dev(self, is_test=False):\n",
    "        return torch.Tensor(\n",
    "            self.get_ratings(is_test).toarray()\n",
    "        ).to(self._device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:27.105565Z",
     "start_time": "2019-12-24T14:59:27.037416Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate(model, data_1, data_2, axis, mode, samples_perc_per_epoch=1):\n",
    "    model.eval()\n",
    "    batch_size = 500\n",
    "    ndcg_dist = []\n",
    "    \n",
    "    \n",
    "    for batch in generate(batch_size=batch_size,\n",
    "                          device=device,\n",
    "                          axis=axis,\n",
    "                          data_1=data_1,\n",
    "                          data_2=data_2,\n",
    "                          samples_perc_per_epoch=samples_perc_per_epoch\n",
    "                         ):\n",
    "        \n",
    "        ratings = batch.get_ratings_to_dev()\n",
    "        idx = batch.get_idx_to_dev()\n",
    "        ratings_test = batch.get_ratings(is_test=True)\n",
    "    \n",
    "        pred_val = model(ratings, idx, calculate_loss=False, mode=mode).cpu().detach().numpy()\n",
    "        \n",
    "        if not (data_1 is data_2):\n",
    "            pred_val[batch.get_ratings().nonzero()] = -np.inf\n",
    "        ndcg_dist.append(utils.NDCG_binary_at_k_batch(pred_val, ratings_test))\n",
    "\n",
    "    ndcg_dist = np.concatenate(ndcg_dist)\n",
    "    return ndcg_dist[~np.isnan(ndcg_dist)].mean()\n",
    "\n",
    "\n",
    "def run(model, opts, train_data, batch_size, n_epochs, axis, beta, mode):\n",
    "    global best_ndcg\n",
    "    global ndcgs_tr_pr, ndcgs_tr_mf, ndcgs_va_pr, ndcgs_va_mf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        NLL_loss = 0\n",
    "        KLD_loss = 0\n",
    "\n",
    "        for batch in generate(batch_size=batch_size, device=device, axis=axis, data_1=train_data, shuffle=True):\n",
    "            ratings = batch.get_ratings_to_dev()\n",
    "            idx = batch.get_idx_to_dev()\n",
    "\n",
    "            for optimizer in opts:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            (NLL, KLD), loss = model(ratings, idx, beta=beta, mode=mode)\n",
    "            loss.backward()\n",
    "            \n",
    "            for optimizer in opts:\n",
    "                optimizer.step()\n",
    "            \n",
    "            NLL_loss += NLL.item()\n",
    "            KLD_loss += KLD.item()\n",
    "            \n",
    "\n",
    "        print('NLL_loss', NLL_loss, 'KLD_loss', KLD_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:59:27.116437Z",
     "start_time": "2019-12-24T14:59:27.106688Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "best_ndcg = -np.inf\n",
    "ndcgs_tr_pr, ndcgs_tr_mf, ndcgs_va_pr, ndcgs_va_mf = [], [], [], []\n",
    "var_param_distance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-24T14:59:22.683Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim = 600\n",
    "latent_dim = 200\n",
    "\n",
    "model_i = VAE(hidden_dim, latent_dim, (n_users, n_items), 'users').to(device)\n",
    "model_i.set_embeddings(train_data)\n",
    "print(model_i)\n",
    "\n",
    "\n",
    "def get_opts(model, lr=5e-4):\n",
    "    decoder_params = set(model.decoder.parameters())\n",
    "    embedding_params = set(model_i.prior.user_mu.parameters()) | set(model_i.prior.user_logvar.parameters())\n",
    "    encoder_params = set(model.parameters()) - decoder_params - embedding_params\n",
    "\n",
    "    optimizer_encoder = optim.Adam(encoder_params, lr=lr)\n",
    "    optimizer_decoder = optim.Adam(decoder_params, lr=lr)\n",
    "    optimizer_embedding = optim.Adam(embedding_params, lr=lr)\n",
    "\n",
    "    print('encoder\\n', [x.shape for x in encoder_params])\n",
    "    print('embedding\\n', [x.shape for x in embedding_params])\n",
    "    print('decoder\\n', [x.shape for x in decoder_params])\n",
    "    \n",
    "    return optimizer_encoder, optimizer_decoder, optimizer_embedding\n",
    "\n",
    "\n",
    "optimizer_encoder_i, optimizer_decoder_i, _ = get_opts(model_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-24T14:59:22.685Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "\n",
    "    run(model_i, [optimizer_encoder_i],\n",
    "        train_data, batch_size=500, n_epochs=3, axis='users', mode='pr', beta=0.005)\n",
    "    model_i.set_embeddings(train_data)\n",
    "    run(model_i, [optimizer_decoder_i],\n",
    "        train_data, batch_size=500, n_epochs=1, axis='users', mode='mf', beta=None)\n",
    "    \n",
    "    model = model_i\n",
    "    axis = 'users'\n",
    "    ndcg_ = validate(model, train_data, train_data, axis, 'mf', 0.01)\n",
    "    ndcgs_tr_mf.append(ndcg_)\n",
    "    ndcg_ = validate(model, train_data, train_data, axis, 'pr', 0.01)\n",
    "    ndcgs_tr_pr.append(ndcg_)\n",
    "    ndcg_ = validate(model, valid_1_data, valid_2_data, axis, 'pr', 1)\n",
    "    ndcgs_va_pr.append(ndcg_)\n",
    "\n",
    "    \n",
    "    clear_output(True)\n",
    "    \n",
    "    i_min = np.array(ndcgs_va_pr).argsort()[-len(ndcgs_va_pr)//2:].min()\n",
    "\n",
    "    print('ndcg', ndcgs_va_pr[-1], ': : :', best_ndcg)\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_size_inches(15,5)\n",
    "\n",
    "    ax1.plot(range(i_min, len(ndcgs_va_pr)), ndcgs_va_pr[i_min:], '+-', label='pr valid')\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(range(i_min, len(ndcgs_va_pr)), ndcgs_tr_pr[i_min:], '+:', label='pr train')\n",
    "    ax2.plot(range(i_min, len(ndcgs_va_pr)), ndcgs_tr_mf[i_min:], 'x:', label='mf train')\n",
    "    ax2.legend(loc='lower left')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.ylabel(\"Validation NDCG@100\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    if ndcg_ > best_ndcg:\n",
    "        best_ndcg = ndcg_\n",
    "        torch.save(model.state_dict(), ser_model_fn)\n",
    "        \n",
    "    if ndcg_ < best_ndcg / 2 and epoch > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-24T14:59:22.686Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "model_i.load_state_dict(torch.load(ser_model_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-24T14:59:22.689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size_test = 2000\n",
    "model_i.eval()\n",
    "n100_list, r20_list, r50_list = [], [], []\n",
    "\n",
    "\n",
    "for batch in generate(batch_size=batch_size_test, device=device, axis='users', data_1=test_1_data, data_2=test_2_data, samples_perc_per_epoch=1):\n",
    "    user_ratings = batch.get_ratings_to_dev()\n",
    "    users_idx = batch.get_idx_to_dev()\n",
    "    user_ratings_test = batch.get_ratings(is_test=True)\n",
    "\n",
    "    pred_val = model_i(user_ratings, users_idx, calculate_loss=False, mode='mf').cpu().detach().numpy()\n",
    "    # exclude examples from training and validation (if any)\n",
    "    pred_val[batch.get_ratings().nonzero()] = -np.inf\n",
    "    n100_list.append(utils.NDCG_binary_at_k_batch(pred_val, user_ratings_test, k=100))\n",
    "    r20_list.append(utils.Recall_at_k_batch(pred_val, user_ratings_test, k=20))\n",
    "    r50_list.append(utils.Recall_at_k_batch(pred_val, user_ratings_test, k=50))\n",
    "    \n",
    "n100_list = np.concatenate(n100_list)\n",
    "r20_list = np.concatenate(r20_list)\n",
    "r50_list = np.concatenate(r50_list)\n",
    "\n",
    "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list[~np.isnan(n100_list)]), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
    "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list[~np.isnan(r20_list)]), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
    "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list[~np.isnan(r50_list)]), np.std(r50_list) / np.sqrt(len(r50_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
