{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>summary</th>\n",
       "      <th>storyline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>47metersdown</td>\n",
       "      <td>Four teen girls diving in a ruined underwater ...</td>\n",
       "      <td>47 Meters Down: Uncaged follows the diving adv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>adogsjourneymovie</td>\n",
       "      <td>A dog finds the meaning of his own existence t...</td>\n",
       "      <td>A dog finds the meaning of his own existence t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>aftermathmovie</td>\n",
       "      <td>Post World War II, a British colonel and his w...</td>\n",
       "      <td>Set in postwar Germany in 1946, Rachael Morgan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>aftermovie</td>\n",
       "      <td>A young woman falls for a guy with a dark secr...</td>\n",
       "      <td>Based on Anna Todd's novel, AFTER follows Tess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>alitamovie</td>\n",
       "      <td>A deactivated cyborg is revived, but cannot re...</td>\n",
       "      <td>Alita is a creation from an age of despair. Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>whatmenwant</td>\n",
       "      <td>A woman is boxed out by the male sports agents...</td>\n",
       "      <td>A woman is boxed out by the male sports agents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>wonderparkmovie</td>\n",
       "      <td>Wonder Park tells the story of an amusement pa...</td>\n",
       "      <td>June, an optimistic, imaginative girl, discove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>xmenmovies</td>\n",
       "      <td>Jean Grey begins to develop incredible powers ...</td>\n",
       "      <td>The X-Men. Protectors of peace. Jean Grey is o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>yardiefilm</td>\n",
       "      <td>British crime drama film directed by Idris Elb...</td>\n",
       "      <td>Set in '70s Kingston and '80s Hackney, Yardie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>yesterdaymovie</td>\n",
       "      <td>A struggling musician realizes he's the only p...</td>\n",
       "      <td>In Lowestoft UK, Jack Malik is a frustrated mu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              username                                            summary  \\\n",
       "0         47metersdown  Four teen girls diving in a ruined underwater ...   \n",
       "1    adogsjourneymovie  A dog finds the meaning of his own existence t...   \n",
       "2       aftermathmovie  Post World War II, a British colonel and his w...   \n",
       "3           aftermovie  A young woman falls for a guy with a dark secr...   \n",
       "4           alitamovie  A deactivated cyborg is revived, but cannot re...   \n",
       "..                 ...                                                ...   \n",
       "106        whatmenwant  A woman is boxed out by the male sports agents...   \n",
       "107    wonderparkmovie  Wonder Park tells the story of an amusement pa...   \n",
       "108         xmenmovies  Jean Grey begins to develop incredible powers ...   \n",
       "109         yardiefilm  British crime drama film directed by Idris Elb...   \n",
       "110     yesterdaymovie  A struggling musician realizes he's the only p...   \n",
       "\n",
       "                                             storyline  \n",
       "0    47 Meters Down: Uncaged follows the diving adv...  \n",
       "1    A dog finds the meaning of his own existence t...  \n",
       "2    Set in postwar Germany in 1946, Rachael Morgan...  \n",
       "3    Based on Anna Todd's novel, AFTER follows Tess...  \n",
       "4    Alita is a creation from an age of despair. Fo...  \n",
       "..                                                 ...  \n",
       "106  A woman is boxed out by the male sports agents...  \n",
       "107  June, an optimistic, imaginative girl, discove...  \n",
       "108  The X-Men. Protectors of peace. Jean Grey is o...  \n",
       "109  Set in '70s Kingston and '80s Hackney, Yardie ...  \n",
       "110  In Lowestoft UK, Jack Malik is a frustrated mu...  \n",
       "\n",
       "[111 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./description.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence remove stopword.... and do some processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#還沒移除標點符號\n",
    "def sentence_processing(sentence):\n",
    "    #stopword get\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_sentence = []\n",
    "    #delete no-English part \n",
    "    if sentence != 'None':\n",
    "        sentence =re.sub('[^a-zA-Z]',' ',sentence)\n",
    "        #tokenization\n",
    "        word_tokens = word_tokenize(sentence) \n",
    "        \"\"\"\n",
    "        #stemming\n",
    "        ps = PorterStemmer()\n",
    "        stemed_words = []\n",
    "        for work_token in word_tokens:\n",
    "            stemed_words.append(ps.stem(work_token))\n",
    "        \"\"\"\n",
    "        #filtered_sentence = [w for w in stemed_words if not w in stop_words] #有stem的狀態下\n",
    "        filtered_sentence = [w.lower() for w in word_tokens if not w in stop_words] #沒有stem的狀態下\n",
    "        #print(stemed_words)\n",
    "        #print(word_tokens) \n",
    "        #print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each description do tokenization, stopword removes, stemming to make a word list for each youtuber \n",
    "doc_list = []\n",
    "for d in data['summary']:\n",
    "    filtered_sentence = sentence_processing(d)\n",
    "    doc_list.append(filtered_sentence)\n",
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec 是 Google 的一個開源工具，能夠根據輸入的「詞的集合」計算出詞與詞之間的距離\n",
    "它將「字詞」轉換成「向量」形式，可以把對文本內容的處理簡化為向量空間中的向量運算，計算出向量空間上的相似度，來表示文本語義上的相似度\n",
    "\n",
    "相關資料 https://code.google.com/archive/p/word2vec/     \n",
    "\n",
    "github  https://github.com/dav/word2vec\n",
    "\n",
    "How to download pre-trained data(Google News dataset) https://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "\n",
    "How to implement word2Vec https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "# Check dimension of word vectors(Each word is represented as a feature vector of 300 dimensions)\n",
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for doc in doc_list:\n",
    "    # Filter the list of vectors to include only those that Word2Vec has a vector for\n",
    "    vector_list = [model[word] for word in doc if word in model.vocab]\n",
    "\n",
    "    # Create a list of the words corresponding to these vectors\n",
    "    words_filtered = [word for word in doc if word in model.vocab]\n",
    "\n",
    "    # Zip the words together with their vector representations\n",
    "    word_vec_zip = zip(words_filtered, vector_list)\n",
    "\n",
    "    # Cast to a dict so we can turn it into a DataFrame\n",
    "    word_vec_dict = dict(word_vec_zip)\n",
    "    df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "    df.to_csv('./feature matrix/'+data['username'][index]+'.csv')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN in keras with pretrained word2vec weights\n",
    "https://www.kaggle.com/marijakekic/cnn-in-keras-with-pretrained-word2vec-weights/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3448651a2695>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNUM_WORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0membedding_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i>=NUM_WORDS:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
